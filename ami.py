from langchain_openai import ChatOpenAI
from knowledge import  retrieve_relevant_infov2 
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
import os
from langchain_openai import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
import json
import logging
from typing import Dict, Any
from openai import OpenAI
logging.basicConfig(level=logging.INFO)
import json


# Initialize conversation memory
memory = ConversationBufferMemory(return_messages=True)

# Declare user_context as a global variable
user_context = {"customer_info": {}}

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "")
PINECONE_ENV = "us-east-1"  # Check Pinecone console for your region
index_name = "ami-knowledge"
llm = ChatOpenAI(model="gpt-4o", streaming=True)
client = OpenAI()
prompt = PromptTemplate(
    input_variables=["history", "user_input", "products", "user_style", "sales_skills"],
    template="""
    üéØ **M·ª•c ti√™u**: Hi·ªÉu √Ω ƒë·ªãnh c·ªßa ng∆∞·ªùi d√πng v√† ph·∫£n h·ªìi m·ªôt c√°ch ph√π h·ª£p.

    1Ô∏è‚É£ **N·∫øu ng∆∞·ªùi d√πng ƒëang h·ªèi v·ªÅ s·∫£n ph·∫©m** ‚Üí D·ª±a v√†o th√¥ng tin s·∫£n ph·∫©m ƒë√£ t√¨m th·∫•y ({products}) ƒë·ªÉ t∆∞ v·∫•n ng·∫Øn g·ªçn, ƒë·ªß √Ω, c√≥ d·∫´n d·∫Øt h·ª£p l√Ω.  
    2Ô∏è‚É£ **N·∫øu ng∆∞·ªùi d√πng ƒëang h·ªèi v·ªÅ k·ªπ nƒÉng b√°n h√†ng** ‚Üí √Åp d·ª•ng k·ªπ nƒÉng ph√π h·ª£p t·ª´ ({sales_skills}) v√†o c√¢u tr·∫£ l·ªùi.  
    3Ô∏è‚É£ **N·∫øu ng∆∞·ªùi d√πng ƒëang tr√≤ chuy·ªán b√¨nh th∆∞·ªùng** ‚Üí Duy tr√¨ h·ªôi tho·∫°i m·ªôt c√°ch t·ª± nhi√™n, c√≥ th·ªÉ th√™m c√¢u h·ªèi g·ª£i m·ªü.  
    4Ô∏è‚É£ **Lu√¥n ph·∫£n h·ªìi theo phong c√°ch c·ªßa ng∆∞·ªùi d√πng tr∆∞·ªõc ƒë√¢y**: {user_style}  

    üìú **L·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán**:  
    {history}  

    üó£ **Tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng**:  
    "{user_input}"  

    ‚úçÔ∏è **Ph·∫£n h·ªìi c·ªßa AMI** (gi·ªØ phong c√°ch h·ªôi tho·∫°i ph√π h·ª£p):  
    """
)

# Initialize chat history
chat_history = ChatMessageHistory()

def retrieve_product(user_input):
    """Retrieve relevant context from Pinecone and return a structured summary."""
    if user_input is None:
        return "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p."  # Return an appropriate message if input is None

    retrieved_info = retrieve_relevant_infov2(user_input, top_k=3)

    if not retrieved_info:
        return "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p."

    structured_summary = []
    for doc in retrieved_info:
        content = doc.get("content", "").strip()
        if content:
            structured_summary.append(content)
    return "\n\n".join(structured_summary) if structured_summary else "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p."

pc = Pinecone(api_key=PINECONE_API_KEY)

# Check if index exists
existing_indexes = [i['name'] for i in pc.list_indexes()]
if index_name not in existing_indexes:
    pc.create_index(
        name=index_name,
        dimension=1536,  # Ensure this matches your model's output dimension
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

def detect_customer_intent_dynamic(message: str) -> Dict[str, Any]:
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": """You are an AI that detects customer intent and categorizes it into three main groups:
                1. general_conversation (e.g., greetings, small talk, general inquiries)
                2. sales_related (e.g., asking about price, product details, promotions)
                3. after_sales (e.g., warranty, support, returns, complaints)
                Return a JSON object with "intent", "intent_group", and optional "sub_intent" fields.
            """},
            {"role": "user", "content": f"Analyze intent from this message: {message}"}
        ]
    )
    intent_data = response.choices[0].message.content.strip()
    
    # Attempt to parse the intent_data as JSON
    try:
        return json.loads(intent_data)  # Use json.loads instead of eval
    except json.JSONDecodeError as e:
        print("JSON decoding error:", e)
        return {"intent": "unknown", "intent_group": "general_conversation"}  # Return a default value in case of error
   
def ami_drive(user_message, user_context,company_goal,product_info):
    """
    X·ª≠ l√Ω tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng, x√°c ƒë·ªãnh m·ª•c ti√™u, t·∫°o Best_map v√† d·∫´n d·∫Øt h·ªôi tho·∫°i.
    """
    intent_data = detect_customer_intent_dynamic(user_message)
    intent = intent_data.get("intent", "unknown")
    intent_group = intent_data.get("intent_group", "general_conversation")  # M·∫∑c ƒë·ªãnh l√† giao ti·∫øp th√¥ng th∆∞·ªùng
    sub_intent = intent_data.get("sub_intent", None)
    print("intent_group in the ami_drive:", intent_group)
    if intent_group == "general_conversation":
        return handle_general_conversation(intent, sub_intent,user_message, user_context)

    elif intent_group == "sales_related":
        return handle_sales(user_message, user_context,company_goal,product_info)

    elif intent_group == "after_sales":
        return "Post-sales support"

    else:
        return "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu r√µ c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n kh√¥ng?"

def handle_general_conversation(intent, sub_intent, user_message, user_context):
    # Load chat history from memory
    chat_history_list = memory.load_memory_variables({})["history"]
    

    # Append the new user message to the chat history
    chat_history_list.append(f"User: {user_message}")
    print(chat_history_list)
    print([type(msg) for msg in chat_history_list])

    # Join the chat history into a single string
    # chat_history_str = "\n".join(chat_history_list)
    # chat_history_str = "\n".join([message.content for message in chat_history_list if hasattr(message, 'content')])
    #chat_history_str = "\n".join(msg.content for msg in chat_history_list)
    chat_history_str = "\n".join(
    msg.content if hasattr(msg, "content") else msg for msg in chat_history_list
    )


    
    # Save the joined chat history to memory
    memory.save_context({"input": "chat_history"}, {"output": chat_history_str})

    # L·∫•y th√¥ng tin kh√°ch h√†ng t·ª´ user_context
    customer_info = user_context.get("customer_info", {})

    # **Danh s√°ch th√¥ng tin c·∫ßn thu th·∫≠p**
    required_fields = ["name", "age", "occupation", "interests"]
    missing_fields = [field for field in required_fields if not customer_info.get(field)]

    # **Ch·ªâ h·ªèi t√™n n·∫øu th·∫≠t s·ª± ch∆∞a c√≥**
    if "name" in missing_fields:
        probing_prompt = f"""
        L·ªãch s·ª≠ h·ªôi tho·∫°i: {chat_history_str}
        Th√¥ng tin kh√°ch h√†ng hi·ªán c√≥: {customer_info}
        B·∫°n l√† m·ªôt tr·ª£ l√Ω AI. Kh√°ch h√†ng ch∆∞a cung c·∫•p t√™n. H√£y ƒë·∫∑t m·ªôt c√¢u h·ªèi l·ªãch s·ª± ƒë·ªÉ h·ªèi t√™n.
        """
        return llm.invoke(probing_prompt).content

    # **N·∫øu ƒë√£ c√≥ t√™n nh∆∞ng c√≤n thi·∫øu th√¥ng tin kh√°c ‚Üí H·ªèi ti·∫øp th√¥ng tin c√≤n thi·∫øu**
    if missing_fields:
        probing_prompt = f"""
        L·ªãch s·ª≠ h·ªôi tho·∫°i: {chat_history_str}
        Th√¥ng tin kh√°ch h√†ng hi·ªán c√≥: {customer_info}
        Th√¥ng tin c√≤n thi·∫øu: {missing_fields}
        H√£y ƒë·∫∑t m·ªôt c√¢u h·ªèi t·ª± nhi√™n ƒë·ªÉ khai th√°c m·ªôt trong c√°c th√¥ng tin c√≤n thi·∫øu m√† kh√¥ng l√†m kh√°ch h√†ng kh√≥ ch·ªãu.
        """
        return llm.invoke(probing_prompt).content

    # **N·∫øu ƒë√£ c√≥ ƒë·ªß th√¥ng tin ‚Üí Tr·∫£ l·ªùi theo ng·ªØ c·∫£nh**
    response_prompt = f"""
    T√≥m t·∫Øt h·ªôi tho·∫°i: {chat_history_str}
    Th√¥ng tin kh√°ch h√†ng: {customer_info}
    C√¢u kh√°ch h√†ng v·ª´a h·ªèi: {user_message}
    H√£y ph·∫£n h·ªìi m·ªôt c√°ch t·ª± nhi√™n, ph√π h·ª£p v·ªõi th√¥ng tin kh√°ch h√†ng, gi·ªØ cu·ªôc tr√≤ chuy·ªán m∆∞·ª£t m√†.
    """
    extract_prompt = f"""
    H·ªôi tho·∫°i: {chat_history_str}
    Th√¥ng tin hi·ªán c√≥: {user_context.get("customer_info", {})}
    H√£y c·∫≠p nh·∫≠t th√¥ng tin kh√°ch h√†ng d·ª±a tr√™n h·ªôi tho·∫°i m·ªõi. 
    Ch√∫ √Ω: N·∫øu ƒë√£ c√≥ th√¥ng tin, kh√¥ng ƒë∆∞·ª£c l√†m m·∫•t th√¥ng tin c≈©. Ch·ªâ b·ªï sung ph·∫ßn c√≤n thi·∫øu.
    """

    return llm.invoke(response_prompt).content
import json

def handle_sales(user_message, user_context, company_goal, product_info):
    """
    X·ª≠ l√Ω tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng, k·∫øt h·ª£p Best Approach v√†o ph·∫£n h·ªìi.
    """

    # Step 1: Retrieve customer info
    customer_info = user_context.get("customer_info", {})
    chat_history_list = memory.load_memory_variables({})["history"]
    chat_history_list.append(f"User: {user_message}")

    # Convert chat history to a string and save
    chat_history_str = "\n".join(
        msg.content if hasattr(msg, "content") else msg for msg in chat_history_list
    )
    memory.save_context({"input": "chat_history"}, {"output": chat_history_str})

    # Step 2: Determine customer stage
    customer_stage = get_customer_stage(chat_history_list)
    user_context["customer_stage"] = customer_stage
    print("customer_stage:", customer_stage)

    next_stop = get_customer_next_stop(customer_stage)
    print("next_stop:", next_stop)

    # Step 3: Identify conversation goal
    convo_goal = get_conversation_goal(customer_info, user_message, customer_stage, next_stop)

    # Step 4: Get Best Approach & Instruction
    approach_data = analyse_approach(customer_stage, convo_goal, customer_info, product_info)

    # üîç Ensure response is valid
    if not approach_data or not isinstance(approach_data, dict):
        print("‚ö†Ô∏è Warning: analyse_approach returned an invalid response!")
        approach_data = {
            "best_approach": "H√£y t·∫°o s·ª± tin t∆∞·ªüng v√† khuy·∫øn kh√≠ch kh√°ch h√†ng.",
            "instruction": "H√£y ph·∫£n h·ªìi l·ªãch s·ª±, t·∫°o s·ª± tin t∆∞·ªüng v√† cung c·∫•p th√™m th√¥ng tin h·ªØu √≠ch."
        }

    best_approach = approach_data.get("best_approach", "H√£y t·∫°o s·ª± tin t∆∞·ªüng v√† khuy·∫øn kh√≠ch kh√°ch h√†ng.")
    instruction = approach_data.get("instruction", "H√£y ph·∫£n h·ªìi l·ªãch s·ª±, t·∫°o s·ª± tin t∆∞·ªüng v√† cung c·∫•p th√™m th√¥ng tin h·ªØu √≠ch.")

    # üîπ Markdown Analysis
    analysis_markdown = f"""
    **üìä Ph√¢n t√≠ch chi·∫øn l∆∞·ª£c:**  
    - **üìç Giai ƒëo·∫°n kh√°ch h√†ng:** {customer_stage}  
    - **üéØ ƒêi·ªÉm ƒë·∫øn ti·∫øp theo:** {next_stop}  
    - **üí° Chi·∫øn thu·∫≠t ti·∫øp c·∫≠n:** {best_approach}  
    - **üí° H∆∞·ªõng d·∫´n ph·∫£n h·ªìi:** {instruction}  

    C√≥ th·ªÉ s·ª≠ d·ª•ng c√¢u d∆∞·ªõi ƒë√¢y:
    ---
    """

    # Step 5: Generate Final Response
    final_response = generate_conversation_response(user_message, customer_info, best_approach, instruction)

    return analysis_markdown + final_response


def generate_conversation_response(user_message, customer_info, best_approach, instruction):
    """
    T·∫°o ph·∫£n h·ªìi h·ªôi tho·∫°i d·ª±a tr√™n Best Approach, Instruction v√† th√¥ng tin kh√°ch h√†ng.
    - Best Approach: h∆∞·ªõng ti·∫øp c·∫≠n ph√π h·ª£p.
    - Instruction: ch·ªâ d·∫´n chi ti·∫øt v·ªÅ c√°ch ph·∫£n h·ªìi.
    """

    print("Best Approach in generate_conversation_response:", best_approach)
    print("Instruction in generate_conversation_response:", instruction)

    prompt = f"""
    üó£Ô∏è Tin nh·∫Øn kh√°ch h√†ng: "{user_message}"
    üë§ Th√¥ng tin kh√°ch h√†ng: {json.dumps(customer_info, ensure_ascii=False)}
    üí° Best Approach: "{best_approach}"
    üéØ Instruction: "{instruction}"

    üîπ H√£y t·∫°o m·ªôt ph·∫£n h·ªìi **t·ª± nhi√™n, th√¢n thi·ªán, g·∫ßn g≈©i**, ph·∫£n √°nh phong c√°ch n√≥i chuy·ªán c·ªßa kh√°ch h√†ng.
    üîπ **T√≠ch h·ª£p Best Approach m·ªôt c√°ch tinh t·∫ø**, kh√¥ng l·∫∑p l·∫°i nguy√™n vƒÉn.
    üîπ **Tu√¢n theo h∆∞·ªõng d·∫´n trong Instruction** ƒë·ªÉ ƒë·∫£m b·∫£o ph·∫£n h·ªìi c√≥ chi·∫øn thu·∫≠t ph√π h·ª£p.
    üîπ ƒê·ª´ng t·∫°o ph·∫£n h·ªìi qu√° d√†i ‚Äì t·ªëi ƒëa 3 c√¢u.
    

    üìù Tr·∫£ l·ªùi:
    """

    response = llm.invoke(prompt)

    if not response or not response.content.strip():
        print("‚ö†Ô∏è LLM response is empty or None")
        return "ƒê√¢y l√† m·ªôt s·∫£n ph·∫©m r·∫•t t·ªët, b·∫°n c√≥ th·ªÉ tham kh·∫£o th√™m nh√©!"

    return response.content.strip()


def generate_response(best_map, next_stop, customer_info):
    """
    Sinh ph·∫£n h·ªìi d·ª±a tr√™n Best_map + h∆∞·ªõng kh√°ch h√†ng ƒë·∫øn ƒë√≠ch ƒë·∫øn.
    """
    print("best_map in generate_response:", best_map)
    prompt = f"""
    Kh√°ch h√†ng: {customer_info}
    Best_map: "{best_map}"
    Company_goal: "{next_stop}"

    H√£y t·∫°o m·ªôt ph·∫£n h·ªìi t·ª± nhi√™n, th√¢n thi·ªán, d·∫´n d·∫Øt kh√°ch h√†ng theo Best_map v√† h∆∞·ªõng h·ªç ƒë·∫øn {next_stop}. H√£y tr·∫£ l·ªùi d√πng ng√¥n ng·ªØ c·ªßa d√πng.
    """

    response = llm.invoke(prompt).content  # G·ªçi OpenAI ho·∫∑c m√¥ h√¨nh AI kh√°c ƒë·ªÉ sinh ph·∫£n h·ªìi
    return response.strip()

def get_customer_stage(chat_history, company_goal="kh√°ch chuy·ªÉn kho·∫£n"):
    """
    D√πng LLM ƒë·ªÉ x√°c ƒë·ªãnh giai ƒëo·∫°n c·ªßa kh√°ch h√†ng d·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i.
    """
    prompt = f"""
    B·∫°n l√† m·ªôt AI t∆∞ v·∫•n b√°n h√†ng. D∆∞·ªõi ƒë√¢y l√† l·ªãch s·ª≠ h·ªôi tho·∫°i gi·ªØa nh√¢n vi√™n v√† kh√°ch h√†ng:
    {chat_history}
    
    C√¥ng ty c√≥ m·ª•c ti√™u cu·ªëi c√πng l√† '{company_goal}'.
    D·ª±a v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i, h√£y x√°c ƒë·ªãnh kh√°ch h√†ng ƒëang ·ªü giai ƒëo·∫°n n√†o trong h√†nh tr√¨nh n√†y:
    - Awareness (Nh·∫≠n th·ª©c)
    - Interest (Quan t√¢m)
    - Consideration (C√¢n nh·∫Øc)
    - Decision (Quy·∫øt ƒë·ªãnh)
    - Action (Chuy·ªÉn kho·∫£n)
    
    Ch·ªâ tr·∫£ v·ªÅ m·ªôt trong c√°c giai ƒëo·∫°n tr√™n m√† kh√¥ng c√≥ b·∫•t k·ª≥ gi·∫£i th√≠ch n√†o.
    """

    response= llm.invoke(prompt).content
    return response.strip()

def get_customer_next_stop(current_stop):
    if "Awareness" in current_stop:
        return "Interest"
    elif "Interest" in current_stop:
        return "Consideration"
    elif "Consideration" in current_stop:
        return "Decision"
    elif "Decision" in current_stop:
        return "Action"
    else:
        return "Unknown Stage"  # Default case to handle unexpected stages
    
def customer_emotion(chat_history):
   
    prompt = f"""
    B·∫°n l√† m·ªôt chuy√™n gia t√¢m l√Ω tinh t·∫ø. H√£y ph√°t hi·ªán c·∫£m x√∫c hi·ªán t·∫°i c·ªßa kh√°ch h√†ng d·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i:
    {chat_history}
    Ch·ªâ tr·∫£ v·ªÅ tr·∫°ng th√°i c·∫£m x√∫c m√† kh√¥ng gi·∫£i th√≠ch th√™m
    """
    response= llm.invoke(prompt).content
    return response.strip()

def get_customer_info():
    """
    D√πng LLM ƒë·ªÉ ph√¢n t√≠ch l·ªãch s·ª≠ h·ªôi tho·∫°i v√† tr√≠ch xu·∫•t th√¥ng tin kh√°ch h√†ng.
    """
    # Load the entire chat history from memory
    chat_history = memory.load_memory_variables({})["history"]
    
    # Extract text from each message in the chat history
    chat_history_str = "\n".join([message.content for message in chat_history if hasattr(message, 'content')])
    print("Formatted chat_history_str:", repr(chat_history_str))

    prompt = f"""
    
    D∆∞·ªõi ƒë√¢y l√† l·ªãch s·ª≠ h·ªôi tho·∫°i gi·ªØa AI v√† kh√°ch h√†ng:
   {chat_history_str}

   H√£y tr√≠ch xu·∫•t c√°c th√¥ng tin sau t·ª´ cu·ªôc tr√≤ chuy·ªán:
   - T√™n kh√°ch h√†ng (name)
   - Tu·ªïi (age)
   - Gi·ªõi t√≠nh (gender)
   - Ngh·ªÅ nghi·ªáp (occupation)
   - S·ªü th√≠ch (interests)
   - L·ªãch s·ª≠ mua h√†ng (purchase_history)
    N·∫øu ch∆∞a c√≥ ƒë·ªß th√¥ng tin, h√£y d·ª± ƒëo√°n d·ª±a tr√™n ng·ªØ c·∫£nh ho·∫∑c ƒë·ªÉ tr·ªëng.

    Tr·∫£ v·ªÅ m·ªôt JSON v·ªõi c√°c tr∆∞·ªùng:
    - name (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
    - age (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
    - gender (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
    - occupation (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
    - interests (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
    - purchase_history (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
    """

    response = llm.invoke(prompt).content  # G·ªçi LLM ƒë·ªÉ ph√¢n t√≠ch
    print("response in the get_customer_info:", response)

    try:
        # Clean the JSON string if it has extra formatting
        json_start = response.find("{")
        json_end = response.rfind("}") + 1
        clean_json = response[json_start:json_end]

        # Ensure the JSON is properly formatted
        clean_json = clean_json.replace("'", '"')  # Replace single quotes with double quotes

        # Parse JSON
        extracted_info = json.loads(clean_json)
        print("Extracted customer info:", extracted_info)

        # Convert extracted_info to a JSON string
        if not extracted_info or all(value == "" for value in extracted_info.values()):
            extracted_info_str = "No customer information extracted."
        else:
            extracted_info_str = json.dumps(extracted_info)

        # Save the context with the string representation
        memory.save_context({"input": "customer_info"}, {"output": extracted_info_str})

        return extracted_info
    except json.JSONDecodeError as e:
        print("JSON decoding error:", e)
        return {}

def update_customer_info(current_info, new_info):
    """
    C·∫≠p nh·∫≠t th√¥ng tin kh√°ch h√†ng v·ªõi d·ªØ li·ªáu m·ªõi m√† kh√¥ng l√†m m·∫•t th√¥ng tin c≈©.
    """
    for key, value in new_info.items():
        if value:  # Ch·ªâ c·∫≠p nh·∫≠t n·∫øu c√≥ th√¥ng tin m·ªõi
            current_info[key] = value
    
    # Ki·ªÉm tra n·∫øu v·∫´n c√≤n missing fields
    missing_fields = [key for key, value in current_info.items() if not value]
    if missing_fields:
        current_info["status"] = "missing_info"
        current_info["missing_fields"] = missing_fields
    else:
        current_info["status"] = "completed"

    return current_info

def get_conversation_goal(customer_info, user_message, customer_stage,next_stop):
    """
    X√°c ƒë·ªãnh m·ª•c ti√™u h·ªôi tho·∫°i d·ª±a tr√™n th√¥ng tin kh√°ch h√†ng, n·ªôi dung tin nh·∫Øn v√† giai ƒëo·∫°n kh√°ch h√†ng.
    """
    print("user_message in the determine_conversation_goal:", user_message)
    print("customer_info in the determine_conversation_goal:", customer_info)
    print("customer_stage in the determine_conversation_goal:", customer_stage)

    # N·∫øu th√¥ng tin kh√°ch c√≤n thi·∫øu, c·∫ßn ti·∫øp t·ª•c h·ªèi ƒë·ªÉ ho√†n ch·ªânh
    # X√°c ƒë·ªãnh m·ª•c ti√™u ti·∫øp theo b·∫±ng c√°ch suy lu·∫≠n t·ª´ company_goal
    prompt = f"""
    D·ª±a tr√™n giai ƒëo·∫°n kh√°ch h√†ng trong h√†nh tr√¨nh mua h√†ng: "{customer_stage}", 
    v√† tin nh·∫Øn: "{user_message}", h√£y x√°c ƒë·ªãnh b∆∞·ªõc h·ª£p l√Ω ti·∫øp theo ƒë·ªÉ d·∫´n kh√°ch h√†ng ƒë·∫øn m·ª•c ti√™u ti·∫øn t·ªõi ƒë∆∞·ª£c {next_stop} .
    
    Tr·∫£ v·ªÅ ch·ªâ m·ªôt m·ª•c ti√™u h·ªôi tho·∫°i c·ª• th·ªÉ (kh√¥ng gi·∫£i th√≠ch), v√≠ d·ª•: "Gi·ªõi thi·ªáu s·∫£n ph·∫©m", "Thuy·∫øt ph·ª•c kh√°ch h√†ng", "H∆∞·ªõng d·∫´n thanh to√°n".
    """

    response = llm.invoke(prompt).content  # G·ªçi LLM ƒë·ªÉ ph√¢n t√≠ch


    return response.strip()


def propose_best_approach(conversation_goal, customer_info, product_info):
    """
    S·ª≠ d·ª•ng LLM ƒë·ªÉ suy lu·∫≠n Best Approach ph√π h·ª£p d·ª±a tr√™n conversation_goal, customer_info v√† product_info.
    """

    print("customer_info in propose_best_approach:", customer_info)

    prompt = f"""
    üèÜ M·ª•c ti√™u h·ªôi tho·∫°i: "{conversation_goal}"
    üë§ Th√¥ng tin kh√°ch h√†ng: {json.dumps(customer_info, ensure_ascii=False)}
    üì¶ Th√¥ng tin s·∫£n ph·∫©m: {json.dumps(product_info, ensure_ascii=False)}

    ‚úÖ H√£y t·∫°o m·ªôt h∆∞·ªõng d·∫´n (Best Approach) gi√∫p nh√¢n vi√™n b√°n h√†ng n√≥i chuy·ªán h·ª£p l√Ω v·ªõi kh√°ch.
    ‚úÖ Best Approach kh√¥ng ph·∫£i l√† c√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp, m√† l√† c√°ch ti·∫øp c·∫≠n t·ªïng quan gi√∫p cu·ªôc tr√≤ chuy·ªán hi·ªáu qu·∫£ h∆°n.

    üîπ Tr·∫£ l·ªùi CH·ªà D∆Ø·ªöI ƒê·ªäNH D·∫†NG JSON nh∆∞ sau:
    ```json
    {{ "best_approach": "<H∆∞·ªõng d·∫´n ng·∫Øn g·ªçn, s√∫c t√≠ch, t·ªëi ƒëa 2 c√¢u>" }}
    ```
    üö´ Kh√¥ng th√™m b·∫•t k·ª≥ vƒÉn b·∫£n n√†o b√™n ngo√†i JSON.
    """

    response = llm.invoke(prompt).content  # G·ªçi LLM

    try:
        # üí° Fix: Clean and parse JSON response
        json_str = response.strip().strip("```json").strip("```").strip()
        best_approach_data = json.loads(json_str)  # Parse cleaned JSON

        if "best_approach" not in best_approach_data:
            raise ValueError("Missing 'best_approach' in response")

        return best_approach_data["best_approach"]

    except Exception as e:
        print(f"‚ö†Ô∏è Error parsing best_approach: {e}, raw response: {response}")
        return "H√£y t·∫°o s·ª± tin t∆∞·ªüng v√† khuy·∫øn kh√≠ch kh√°ch h√†ng."  # Fallback approach
import json
import re
import json
import re

def analyse_approach(customer_stage,conversation_goal, customer_info, product_info):
    """
    S·ª≠ d·ª•ng LLM ƒë·ªÉ suy lu·∫≠n chi·∫øn thu·∫≠t ti·∫øp c·∫≠n kh√°ch h√†ng v√† t·∫°o h∆∞·ªõng d·∫´n cho response prompt.
    
    üìå Output g·ªìm:
    - best_approach: C√°ch ti·∫øp c·∫≠n ng·∫Øn g·ªçn ƒë·ªÉ ƒë·∫°t conversation_goal.
    - instruction: H∆∞·ªõng d·∫´n c·ª• th·ªÉ ƒë·ªÉ truy·ªÅn v√†o response prompt.
    """

    print("customer_info in analyse_approach:", customer_info)

    prompt = f"""
    üèÜ M·ª•c ti√™u h·ªôi tho·∫°i: "{conversation_goal}"
    üìå Giai ƒëo·∫°n kh√°ch h√†ng: "{customer_stage}"
    üë§ Th√¥ng tin kh√°ch h√†ng: {json.dumps(customer_info, ensure_ascii=False)}
    üì¶ Th√¥ng tin s·∫£n ph·∫©m: {json.dumps(product_info, ensure_ascii=False)}

    ‚úÖ H√£y ph√¢n t√≠ch hi·ªán tr·∫°ng kh√°ch h√†ng v√† ƒë·ªÅ xu·∫•t c√°ch ti·∫øp c·∫≠n hi·ªáu qu·∫£ ƒë·ªÉ ƒë·∫°t m·ª•c ti√™u h·ªôi tho·∫°i.
    ‚úÖ Sau ƒë√≥, t·∫°o h∆∞·ªõng d·∫´n (instruction) gi√∫p AI sinh ra ph·∫£n h·ªìi h·ª£p l√Ω trong cu·ªôc tr√≤ chuy·ªán.

    üîπ Tr·∫£ l·ªùi CH·ªà D∆Ø·ªöI ƒê·ªäNH D·∫†NG JSON nh∆∞ sau:
    ```json
    {{
        "best_approach": "<H∆∞·ªõng d·∫´n ti·∫øp c·∫≠n ng·∫Øn g·ªçn, t·ªëi ƒëa 2 c√¢u>",
        "instruction": "<H∆∞·ªõng d·∫´n chi ti·∫øt ƒë·ªÉ truy·ªÅn v√†o response prompt>"
    }}
    ```
    üö´ Kh√¥ng th√™m b·∫•t k·ª≥ vƒÉn b·∫£n n√†o b√™n ngo√†i JSON.
    """

    response = llm.invoke(prompt)

    if not response or not response.content:
        print("‚ö†Ô∏è LLM response is empty or None")
        return {
            "best_approach": "H√£y t·∫°o s·ª± tin t∆∞·ªüng v√† khuy·∫øn kh√≠ch kh√°ch h√†ng.",
            "instruction": "H√£y ph·∫£n h·ªìi l·ªãch s·ª±, t·∫°o s·ª± tin t∆∞·ªüng v√† cung c·∫•p th√™m th√¥ng tin h·ªØu √≠ch."
        }

    raw_response = response.content.strip()
    
    # üí° S·ª≠ d·ª•ng regex ƒë·ªÉ l·∫•y JSON ch√≠nh x√°c (ph√≤ng khi LLM tr·∫£ v·ªÅ text l·∫´n JSON)
    match = re.search(r'\{.*\}', raw_response, re.DOTALL)

    if not match:
        print(f"‚ö†Ô∏è No valid JSON found in response: {raw_response}")
        return {
            "best_approach": "H√£y t·∫°o s·ª± tin t∆∞·ªüng v√† khuy·∫øn kh√≠ch kh√°ch h√†ng.",
            "instruction": "H√£y ph·∫£n h·ªìi l·ªãch s·ª±, t·∫°o s·ª± tin t∆∞·ªüng v√† cung c·∫•p th√™m th√¥ng tin h·ªØu √≠ch."
        }

    json_str = match.group(0)

    try:
        result = json.loads(json_str)  # Parse JSON

        if "best_approach" not in result or "instruction" not in result:
            raise ValueError("Missing keys in JSON response")

        return result

    except Exception as e:
        print(f"‚ö†Ô∏è JSON parsing error: {e}, raw response: {json_str}")
        return {
            "best_approach": "H√£y t·∫°o s·ª± tin t∆∞·ªüng v√† khuy·∫øn kh√≠ch kh√°ch h√†ng.",
            "instruction": "H√£y ph·∫£n h·ªìi l·ªãch s·ª±, t·∫°o s·ª± tin t∆∞·ªüng v√† cung c·∫•p th√™m th√¥ng tin h·ªØu √≠ch."
        }

def search_sales_skills(query_text, max_skills=3):
    """ 
    Truy v·∫•n k·ªπ nƒÉng t·ª´ Pinecone v·ªõi ƒë·ªô ch√≠nh x√°c cao h∆°n. 
    """
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-large", dimensions=1536)
    query_embedding = embedding_model.embed_query(query_text)

    index = pc.Index(index_name)
    response = index.query(
        vector=query_embedding,
        top_k=max_skills,
        include_metadata=True  
    )

    skills = []
    if response and "matches" in response:
        for match in response["matches"]:
            skill_text = match.get("metadata", {}).get("content")
            if skill_text:
                skills.append(skill_text)

    return skills if skills else ["Kh√¥ng t√¨m th·∫•y k·ªπ nƒÉng ph√π h·ª£p."]



chain = (
    RunnablePassthrough.assign(
        history=lambda _: memory.load_memory_variables({}).get("history", []),
        products=lambda x: retrieve_product(x["user_input"]),
        sales_skills=lambda x: ", ".join(search_sales_skills(x["user_input"], max_skills=3)),  # L·∫•y k·ªπ nƒÉng t·ª´ Pinecone
        user_style=lambda _: "l·ªãch s·ª±"
    )  
    | prompt
    | llm
)

def ami_selling(user_message):
    """
    H√†m ch√≠nh x·ª≠ l√Ω h·ªôi tho·∫°i b√°n h√†ng c·ªßa AMI.
    """
    global memory  # Ensure we are using the global memory instance

    # Save the user message to memory
    memory.save_context({"input": user_message}, {"output": ""})

    # Load the entire chat history
    chat_history_list = memory.load_memory_variables({})["history"]

    
    # Extract customer information from the chat history
    extracted_info = get_customer_info()
    
    # Update memory with extracted customer information (if needed)
    if extracted_info:
        extracted_info_str = json.dumps(extracted_info)
        memory.save_context({"input": "customer_info"}, {"output": extracted_info_str})
        user_context["customer_info"] = extracted_info
    else:
        memory.save_context({"input": "customer_info"}, {"output": "No customer information extracted."})

    print("Updated user_context:", user_context)

    company_goal = "Kh√°ch chuy·ªÉn kho·∫£n"
    product_info = retrieve_product(user_message)

    # G·ªçi handle_user_message ƒë·ªÉ l·∫•y ph·∫£n h·ªìi ch√≠nh theo Best_map
    response = ami_drive(user_message, user_context, company_goal, product_info)

    return response

