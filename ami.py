from langchain_openai import ChatOpenAI
from knowledge import  retrieve_relevant_infov2 
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
import os
from langchain_openai import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
import json
import logging
from typing import Dict, Any
from openai import OpenAI
logging.basicConfig(level=logging.INFO)
import json


# Initialize conversation memory
memory = ConversationBufferMemory(return_messages=True)

# Declare user_context as a global variable
user_context = {"customer_info": {}}

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "")
PINECONE_ENV = "us-east-1"  # Check Pinecone console for your region
index_name = "ami-knowledge"
llm = ChatOpenAI(model="gpt-4o", streaming=True)
client = OpenAI()
prompt = PromptTemplate(
    input_variables=["history", "user_input", "products", "user_style", "sales_skills"],
    template="""
    ğŸ¯ **Má»¥c tiÃªu**: Hiá»ƒu Ã½ Ä‘á»‹nh cá»§a ngÆ°á»i dÃ¹ng vÃ  pháº£n há»“i má»™t cÃ¡ch phÃ¹ há»£p.

    1ï¸âƒ£ **Náº¿u ngÆ°á»i dÃ¹ng Ä‘ang há»i vá» sáº£n pháº©m** â†’ Dá»±a vÃ o thÃ´ng tin sáº£n pháº©m Ä‘Ã£ tÃ¬m tháº¥y ({products}) Ä‘á»ƒ tÆ° váº¥n ngáº¯n gá»n, Ä‘á»§ Ã½, cÃ³ dáº«n dáº¯t há»£p lÃ½.  
    2ï¸âƒ£ **Náº¿u ngÆ°á»i dÃ¹ng Ä‘ang há»i vá» ká»¹ nÄƒng bÃ¡n hÃ ng** â†’ Ãp dá»¥ng ká»¹ nÄƒng phÃ¹ há»£p tá»« ({sales_skills}) vÃ o cÃ¢u tráº£ lá»i.  
    3ï¸âƒ£ **Náº¿u ngÆ°á»i dÃ¹ng Ä‘ang trÃ² chuyá»‡n bÃ¬nh thÆ°á»ng** â†’ Duy trÃ¬ há»™i thoáº¡i má»™t cÃ¡ch tá»± nhiÃªn, cÃ³ thá»ƒ thÃªm cÃ¢u há»i gá»£i má»Ÿ.  
    4ï¸âƒ£ **LuÃ´n pháº£n há»“i theo phong cÃ¡ch cá»§a ngÆ°á»i dÃ¹ng trÆ°á»›c Ä‘Ã¢y**: {user_style}  

    ğŸ“œ **Lá»‹ch sá»­ cuá»™c trÃ² chuyá»‡n**:  
    {history}  

    ğŸ—£ **Tin nháº¯n tá»« ngÆ°á»i dÃ¹ng**:  
    "{user_input}"  

    âœï¸ **Pháº£n há»“i cá»§a AMI** (giá»¯ phong cÃ¡ch há»™i thoáº¡i phÃ¹ há»£p):  
    """
)

# Initialize chat history
chat_history = ChatMessageHistory()

def retrieve_product(user_input):
    """Retrieve relevant context from Pinecone and return a structured summary."""
    if user_input is None:
        return "KhÃ´ng tÃ¬m tháº¥y sáº£n pháº©m phÃ¹ há»£p."  # Return an appropriate message if input is None

    retrieved_info = retrieve_relevant_infov2(user_input, top_k=3)

    if not retrieved_info:
        return "KhÃ´ng tÃ¬m tháº¥y sáº£n pháº©m phÃ¹ há»£p."

    structured_summary = []
    for doc in retrieved_info:
        content = doc.get("content", "").strip()
        if content:
            structured_summary.append(content)
    return "\n\n".join(structured_summary) if structured_summary else "KhÃ´ng tÃ¬m tháº¥y sáº£n pháº©m phÃ¹ há»£p."

pc = Pinecone(api_key=PINECONE_API_KEY)

# Check if index exists
existing_indexes = [i['name'] for i in pc.list_indexes()]
if index_name not in existing_indexes:
    pc.create_index(
        name=index_name,
        dimension=1536,  # Ensure this matches your model's output dimension
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

def detect_customer_intent_dynamic(message: str) -> Dict[str, Any]:
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": """You are an AI that detects customer intent and categorizes it into three main groups:
                1. general_conversation (e.g., greetings, small talk, general inquiries)
                2. sales_related (e.g., asking about price, product details, promotions)
                3. after_sales (e.g., warranty, support, returns, complaints)
                Return a JSON object with "intent", "intent_group", and optional "sub_intent" fields.
            """},
            {"role": "user", "content": f"Analyze intent from this message: {message}"}
        ]
    )
    intent_data = response.choices[0].message.content.strip()
    
    # Attempt to parse the intent_data as JSON
    try:
        return json.loads(intent_data)  # Use json.loads instead of eval
    except json.JSONDecodeError as e:
        print("JSON decoding error:", e)
        return {"intent": "unknown", "intent_group": "general_conversation"}  # Return a default value in case of error
   
def ami_drive(user_message, user_context,company_goal,product_info):
    """
    Xá»­ lÃ½ tin nháº¯n tá»« ngÆ°á»i dÃ¹ng, xÃ¡c Ä‘á»‹nh má»¥c tiÃªu, táº¡o Best_map vÃ  dáº«n dáº¯t há»™i thoáº¡i.
    """
    intent_data = detect_customer_intent_dynamic(user_message)
    intent = intent_data.get("intent", "unknown")
    intent_group = intent_data.get("intent_group", "general_conversation")  # Máº·c Ä‘á»‹nh lÃ  giao tiáº¿p thÃ´ng thÆ°á»ng
    sub_intent = intent_data.get("sub_intent", None)
    print("intent_group in the ami_drive:", intent_group)
    if intent_group == "general_conversation":
        return handle_general_conversation(intent, sub_intent,user_message, user_context)

    elif intent_group == "sales_related":
        return handle_sales(user_message, user_context,company_goal,product_info)

    elif intent_group == "after_sales":
        return "Post-sales support"

    else:
        return "Xin lá»—i, tÃ´i chÆ°a hiá»ƒu rÃµ cÃ¢u há»i cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n khÃ´ng?"

def handle_general_conversation(intent, sub_intent, user_message, user_context):
    # Load chat history from memory
    chat_history_list = memory.load_memory_variables({})["history"]
    

    # Append the new user message to the chat history
    chat_history_list.append(f"User: {user_message}")
    print(chat_history_list)
    print([type(msg) for msg in chat_history_list])

    # Join the chat history into a single string
    # chat_history_str = "\n".join(chat_history_list)
    # chat_history_str = "\n".join([message.content for message in chat_history_list if hasattr(message, 'content')])
    #chat_history_str = "\n".join(msg.content for msg in chat_history_list)
    chat_history_str = "\n".join(
    msg.content if hasattr(msg, "content") else msg for msg in chat_history_list
    )


    
    # Save the joined chat history to memory
    memory.save_context({"input": "chat_history"}, {"output": chat_history_str})

    # Láº¥y thÃ´ng tin khÃ¡ch hÃ ng tá»« user_context
    customer_info = user_context.get("customer_info", {})

    # **Danh sÃ¡ch thÃ´ng tin cáº§n thu tháº­p**
    required_fields = ["name", "age", "occupation", "interests"]
    missing_fields = [field for field in required_fields if not customer_info.get(field)]

    # **Chá»‰ há»i tÃªn náº¿u tháº­t sá»± chÆ°a cÃ³**
    if "name" in missing_fields:
        probing_prompt = f"""
        Lá»‹ch sá»­ há»™i thoáº¡i: {chat_history_str}
        ThÃ´ng tin khÃ¡ch hÃ ng hiá»‡n cÃ³: {customer_info}
        Báº¡n lÃ  má»™t trá»£ lÃ½ AI. KhÃ¡ch hÃ ng chÆ°a cung cáº¥p tÃªn. HÃ£y Ä‘áº·t má»™t cÃ¢u há»i lá»‹ch sá»± Ä‘á»ƒ há»i tÃªn.
        """
        return llm.invoke(probing_prompt).content

    # **Náº¿u Ä‘Ã£ cÃ³ tÃªn nhÆ°ng cÃ²n thiáº¿u thÃ´ng tin khÃ¡c â†’ Há»i tiáº¿p thÃ´ng tin cÃ²n thiáº¿u**
    if missing_fields:
        probing_prompt = f"""
        Lá»‹ch sá»­ há»™i thoáº¡i: {chat_history_str}
        ThÃ´ng tin khÃ¡ch hÃ ng hiá»‡n cÃ³: {customer_info}
        ThÃ´ng tin cÃ²n thiáº¿u: {missing_fields}
        HÃ£y Ä‘áº·t má»™t cÃ¢u há»i tá»± nhiÃªn Ä‘á»ƒ khai thÃ¡c má»™t trong cÃ¡c thÃ´ng tin cÃ²n thiáº¿u mÃ  khÃ´ng lÃ m khÃ¡ch hÃ ng khÃ³ chá»‹u.
        """
        return llm.invoke(probing_prompt).content

    # **Náº¿u Ä‘Ã£ cÃ³ Ä‘á»§ thÃ´ng tin â†’ Tráº£ lá»i theo ngá»¯ cáº£nh**
    response_prompt = f"""
    TÃ³m táº¯t há»™i thoáº¡i: {chat_history_str}
    ThÃ´ng tin khÃ¡ch hÃ ng: {customer_info}
    CÃ¢u khÃ¡ch hÃ ng vá»«a há»i: {user_message}
    HÃ£y pháº£n há»“i má»™t cÃ¡ch tá»± nhiÃªn, phÃ¹ há»£p vá»›i thÃ´ng tin khÃ¡ch hÃ ng, giá»¯ cuá»™c trÃ² chuyá»‡n mÆ°á»£t mÃ .
    """
    extract_prompt = f"""
    Há»™i thoáº¡i: {chat_history_str}
    ThÃ´ng tin hiá»‡n cÃ³: {user_context.get("customer_info", {})}
    HÃ£y cáº­p nháº­t thÃ´ng tin khÃ¡ch hÃ ng dá»±a trÃªn há»™i thoáº¡i má»›i. 
    ChÃº Ã½: Náº¿u Ä‘Ã£ cÃ³ thÃ´ng tin, khÃ´ng Ä‘Æ°á»£c lÃ m máº¥t thÃ´ng tin cÅ©. Chá»‰ bá»• sung pháº§n cÃ²n thiáº¿u.
    """

    return llm.invoke(response_prompt).content
import json

def handle_sales(user_message, user_context, company_goal, product_info):
    """
    Xá»­ lÃ½ tin nháº¯n tá»« ngÆ°á»i dÃ¹ng, káº¿t há»£p Best Approach vÃ o pháº£n há»“i.
    """

    # Step 1: Retrieve customer info
    customer_info = user_context.get("customer_info", {})
    chat_history_list = memory.load_memory_variables({})["history"]
    chat_history_list.append(f"User: {user_message}")

    # Convert chat history to a string and save
    chat_history_str = "\n".join(
        msg.content if hasattr(msg, "content") else msg for msg in chat_history_list
    )
    memory.save_context({"input": "chat_history"}, {"output": chat_history_str})

    # Step 2: Determine customer stage
    customer_stage = get_customer_stage(chat_history_list)
    user_context["customer_stage"] = customer_stage
    print("customer_stage:", customer_stage)

    next_stop = get_customer_next_stop(customer_stage)
    print("next_stop:", next_stop)

    # Step 3: Identify conversation goal
    convo_goal = get_conversation_goal(customer_info, user_message, customer_stage, next_stop)

    # Step 4: Get Best Approach & Instruction
    approach_data = analyse_approach(customer_stage, convo_goal, customer_info, product_info)

    # ğŸ” Ensure response is valid
    if not approach_data or not isinstance(approach_data, dict):
        print("âš ï¸ Warning: analyse_approach returned an invalid response!")
        approach_data = {
            "best_approach": "HÃ£y táº¡o sá»± tin tÆ°á»Ÿng vÃ  khuyáº¿n khÃ­ch khÃ¡ch hÃ ng.",
            "instruction": "HÃ£y pháº£n há»“i lá»‹ch sá»±, táº¡o sá»± tin tÆ°á»Ÿng vÃ  cung cáº¥p thÃªm thÃ´ng tin há»¯u Ã­ch."
        }

    best_approach = approach_data.get("best_approach", "HÃ£y táº¡o sá»± tin tÆ°á»Ÿng vÃ  khuyáº¿n khÃ­ch khÃ¡ch hÃ ng.")
    instruction = approach_data.get("instruction", "HÃ£y pháº£n há»“i lá»‹ch sá»±, táº¡o sá»± tin tÆ°á»Ÿng vÃ  cung cáº¥p thÃªm thÃ´ng tin há»¯u Ã­ch.")

    # ğŸ”¹ Markdown Analysis
    analysis_markdown = f"""
    **ğŸ“Š PhÃ¢n tÃ­ch chiáº¿n lÆ°á»£c:**  
    - **ğŸ“ Giai Ä‘oáº¡n khÃ¡ch hÃ ng:** {customer_stage}  
    - **ğŸ¯ Äiá»ƒm Ä‘áº¿n tiáº¿p theo:** {next_stop}  
    - **ğŸ’¡ Chiáº¿n thuáº­t tiáº¿p cáº­n:** {best_approach}  
    - **ğŸ’¡ HÆ°á»›ng dáº«n pháº£n há»“i:** {instruction}  

    CÃ³ thá»ƒ sá»­ dá»¥ng cÃ¢u dÆ°á»›i Ä‘Ã¢y:
    ---
    """

    # Step 5: Generate Final Response
    final_response = generate_conversation_response(user_message, customer_info, best_approach, instruction)

    return analysis_markdown + final_response


def generate_conversation_response(user_message, customer_info, best_approach, instruction):
    """
    Táº¡o pháº£n há»“i há»™i thoáº¡i dá»±a trÃªn Best Approach, Instruction vÃ  thÃ´ng tin khÃ¡ch hÃ ng.
    - Best Approach: hÆ°á»›ng tiáº¿p cáº­n phÃ¹ há»£p.
    - Instruction: chá»‰ dáº«n chi tiáº¿t vá» cÃ¡ch pháº£n há»“i.
    """

    print("Best Approach in generate_conversation_response:", best_approach)
    print("Instruction in generate_conversation_response:", instruction)

    prompt = f"""
    ğŸ—£ï¸ Tin nháº¯n khÃ¡ch hÃ ng: "{user_message}"
    ğŸ‘¤ ThÃ´ng tin khÃ¡ch hÃ ng: {json.dumps(customer_info, ensure_ascii=False)}
    ğŸ’¡ Best Approach: "{best_approach}"
    ğŸ¯ Instruction: "{instruction}"

    ğŸ”¹ HÃ£y táº¡o má»™t pháº£n há»“i **tá»± nhiÃªn, thÃ¢n thiá»‡n, gáº§n gÅ©i**, pháº£n Ã¡nh phong cÃ¡ch nÃ³i chuyá»‡n cá»§a khÃ¡ch hÃ ng.
    ğŸ”¹ **TÃ­ch há»£p Best Approach má»™t cÃ¡ch tinh táº¿**, khÃ´ng láº·p láº¡i nguyÃªn vÄƒn.
    ğŸ”¹ **TuÃ¢n theo hÆ°á»›ng dáº«n trong Instruction** Ä‘á»ƒ Ä‘áº£m báº£o pháº£n há»“i cÃ³ chiáº¿n thuáº­t phÃ¹ há»£p.
    ğŸ”¹ Äá»«ng táº¡o pháº£n há»“i quÃ¡ dÃ i â€“ tá»‘i Ä‘a 3 cÃ¢u.
    

    ğŸ“ Tráº£ lá»i:
    """

    response = llm.invoke(prompt)

    if not response or not response.content.strip():
        print("âš ï¸ LLM response is empty or None")
        return "ÄÃ¢y lÃ  má»™t sáº£n pháº©m ráº¥t tá»‘t, báº¡n cÃ³ thá»ƒ tham kháº£o thÃªm nhÃ©!"

    return response.content.strip()


def generate_response(best_map, next_stop, customer_info):
    """
    Sinh pháº£n há»“i dá»±a trÃªn Best_map + hÆ°á»›ng khÃ¡ch hÃ ng Ä‘áº¿n Ä‘Ã­ch Ä‘áº¿n.
    """
    print("best_map in generate_response:", best_map)
    prompt = f"""
    KhÃ¡ch hÃ ng: {customer_info}
    Best_map: "{best_map}"
    Company_goal: "{next_stop}"

    HÃ£y táº¡o má»™t pháº£n há»“i tá»± nhiÃªn, thÃ¢n thiá»‡n, dáº«n dáº¯t khÃ¡ch hÃ ng theo Best_map vÃ  hÆ°á»›ng há» Ä‘áº¿n {next_stop}. HÃ£y tráº£ lá»i dÃ¹ng ngÃ´n ngá»¯ cá»§a dÃ¹ng.
    """

    response = llm.invoke(prompt).content  # Gá»i OpenAI hoáº·c mÃ´ hÃ¬nh AI khÃ¡c Ä‘á»ƒ sinh pháº£n há»“i
    return response.strip()

def get_customer_stage(chat_history, company_goal="khÃ¡ch chuyá»ƒn khoáº£n"):
    """
    DÃ¹ng LLM Ä‘á»ƒ xÃ¡c Ä‘á»‹nh giai Ä‘oáº¡n cá»§a khÃ¡ch hÃ ng dá»±a trÃªn lá»‹ch sá»­ há»™i thoáº¡i.
    """
    prompt = f"""
    Báº¡n lÃ  má»™t AI tÆ° váº¥n bÃ¡n hÃ ng. DÆ°á»›i Ä‘Ã¢y lÃ  lá»‹ch sá»­ há»™i thoáº¡i giá»¯a nhÃ¢n viÃªn vÃ  khÃ¡ch hÃ ng:
    {chat_history}
    
    CÃ´ng ty cÃ³ má»¥c tiÃªu cuá»‘i cÃ¹ng lÃ  '{company_goal}'.
    Dá»±a vÃ o lá»‹ch sá»­ há»™i thoáº¡i, hÃ£y xÃ¡c Ä‘á»‹nh khÃ¡ch hÃ ng Ä‘ang á»Ÿ giai Ä‘oáº¡n nÃ o trong hÃ nh trÃ¬nh nÃ y:
    - Awareness (Nháº­n thá»©c)
    - Interest (Quan tÃ¢m)
    - Consideration (CÃ¢n nháº¯c)
    - Decision (Quyáº¿t Ä‘á»‹nh)
    - Action (Chuyá»ƒn khoáº£n)
    
    Chá»‰ tráº£ vá» má»™t trong cÃ¡c giai Ä‘oáº¡n trÃªn mÃ  khÃ´ng cÃ³ báº¥t ká»³ giáº£i thÃ­ch nÃ o.
    """

    response= llm.invoke(prompt).content
    return response.strip()

def get_customer_next_stop(current_stop):
    if "Awareness" in current_stop:
        return "Interest"
    elif "Interest" in current_stop:
        return "Consideration"
    elif "Consideration" in current_stop:
        return "Decision"
    elif "Decision" in current_stop:
        return "Action"
    else:
        return "Unknown Stage"  # Default case to handle unexpected stages
    
def customer_emotion(chat_history):
   
    prompt = f"""
    Báº¡n lÃ  má»™t chuyÃªn gia tÃ¢m lÃ½ tinh táº¿. HÃ£y phÃ¡t hiá»‡n cáº£m xÃºc hiá»‡n táº¡i cá»§a khÃ¡ch hÃ ng dá»±a trÃªn lá»‹ch sá»­ há»™i thoáº¡i:
    {chat_history}
    Chá»‰ tráº£ vá» tráº¡ng thÃ¡i cáº£m xÃºc mÃ  khÃ´ng giáº£i thÃ­ch thÃªm
    """
    response= llm.invoke(prompt).content
    return response.strip()

def get_customer_info():
    """
    DÃ¹ng LLM Ä‘á»ƒ phÃ¢n tÃ­ch lá»‹ch sá»­ há»™i thoáº¡i vÃ  trÃ­ch xuáº¥t thÃ´ng tin khÃ¡ch hÃ ng.
    """
    # Load the entire chat history from memory
    chat_history = memory.load_memory_variables({})["history"]
    
    # Extract text from each message in the chat history
    chat_history_str = "\n".join([message.content for message in chat_history if hasattr(message, 'content')])
    print("Formatted chat_history_str:", repr(chat_history_str))

    prompt = f"""
    
    DÆ°á»›i Ä‘Ã¢y lÃ  lá»‹ch sá»­ há»™i thoáº¡i giá»¯a AI vÃ  khÃ¡ch hÃ ng:
   {chat_history_str}

   HÃ£y trÃ­ch xuáº¥t cÃ¡c thÃ´ng tin sau tá»« cuá»™c trÃ² chuyá»‡n:
   - TÃªn khÃ¡ch hÃ ng (name)
   - Tuá»•i (age)
   - Giá»›i tÃ­nh (gender)
   - Nghá» nghiá»‡p (occupation)
   - Sá»Ÿ thÃ­ch (interests)
   - Lá»‹ch sá»­ mua hÃ ng (purchase_history)
    Náº¿u chÆ°a cÃ³ Ä‘á»§ thÃ´ng tin, hÃ£y dá»± Ä‘oÃ¡n dá»±a trÃªn ngá»¯ cáº£nh hoáº·c Ä‘á»ƒ trá»‘ng.

    Tráº£ vá» má»™t JSON vá»›i cÃ¡c trÆ°á»ng:
    - name (náº¿u cÃ³ thá»ƒ suy luáº­n)
    - age (náº¿u cÃ³ thá»ƒ suy luáº­n)
    - gender (náº¿u cÃ³ thá»ƒ suy luáº­n)
    - occupation (náº¿u cÃ³ thá»ƒ suy luáº­n)
    - interests (náº¿u cÃ³ thá»ƒ suy luáº­n)
    - purchase_history (náº¿u cÃ³ thá»ƒ suy luáº­n)
    """

    response = llm.invoke(prompt).content  # Gá»i LLM Ä‘á»ƒ phÃ¢n tÃ­ch
    print("response in the get_customer_info:", response)

    try:
        # Clean the JSON string if it has extra formatting
        json_start = response.find("{")
        json_end = response.rfind("}") + 1
        clean_json = response[json_start:json_end]

        # Ensure the JSON is properly formatted
        clean_json = clean_json.replace("'", '"')  # Replace single quotes with double quotes

        # Parse JSON
        extracted_info = json.loads(clean_json)
        print("Extracted customer info:", extracted_info)

        # Convert extracted_info to a JSON string
        if not extracted_info or all(value == "" for value in extracted_info.values()):
            extracted_info_str = "No customer information extracted."
        else:
            extracted_info_str = json.dumps(extracted_info)

        # Save the context with the string representation
        memory.save_context({"input": "customer_info"}, {"output": extracted_info_str})

        return extracted_info
    except json.JSONDecodeError as e:
        print("JSON decoding error:", e)
        return {}

def update_customer_info(current_info, new_info):
    """
    Cáº­p nháº­t thÃ´ng tin khÃ¡ch hÃ ng vá»›i dá»¯ liá»‡u má»›i mÃ  khÃ´ng lÃ m máº¥t thÃ´ng tin cÅ©.
    """
    for key, value in new_info.items():
        if value:  # Chá»‰ cáº­p nháº­t náº¿u cÃ³ thÃ´ng tin má»›i
            current_info[key] = value
    
    # Kiá»ƒm tra náº¿u váº«n cÃ²n missing fields
    missing_fields = [key for key, value in current_info.items() if not value]
    if missing_fields:
        current_info["status"] = "missing_info"
        current_info["missing_fields"] = missing_fields
    else:
        current_info["status"] = "completed"

    return current_info

def get_conversation_goal(customer_info, user_message, customer_stage,next_stop):
    """
    XÃ¡c Ä‘á»‹nh má»¥c tiÃªu há»™i thoáº¡i dá»±a trÃªn thÃ´ng tin khÃ¡ch hÃ ng, ná»™i dung tin nháº¯n vÃ  giai Ä‘oáº¡n khÃ¡ch hÃ ng.
    """
    print("user_message in the determine_conversation_goal:", user_message)
    print("customer_info in the determine_conversation_goal:", customer_info)
    print("customer_stage in the determine_conversation_goal:", customer_stage)

    # Náº¿u thÃ´ng tin khÃ¡ch cÃ²n thiáº¿u, cáº§n tiáº¿p tá»¥c há»i Ä‘á»ƒ hoÃ n chá»‰nh
    # XÃ¡c Ä‘á»‹nh má»¥c tiÃªu tiáº¿p theo báº±ng cÃ¡ch suy luáº­n tá»« company_goal
    prompt = f"""
    Dá»±a trÃªn giai Ä‘oáº¡n khÃ¡ch hÃ ng trong hÃ nh trÃ¬nh mua hÃ ng: "{customer_stage}", 
    vÃ  tin nháº¯n: "{user_message}", hÃ£y xÃ¡c Ä‘á»‹nh bÆ°á»›c há»£p lÃ½ tiáº¿p theo Ä‘á»ƒ dáº«n khÃ¡ch hÃ ng Ä‘áº¿n má»¥c tiÃªu tiáº¿n tá»›i Ä‘Æ°á»£c {next_stop} .
    
    Tráº£ vá» chá»‰ má»™t má»¥c tiÃªu há»™i thoáº¡i cá»¥ thá»ƒ (khÃ´ng giáº£i thÃ­ch), vÃ­ dá»¥: "Giá»›i thiá»‡u sáº£n pháº©m", "Thuyáº¿t phá»¥c khÃ¡ch hÃ ng", "HÆ°á»›ng dáº«n thanh toÃ¡n".
    """

    response = llm.invoke(prompt).content  # Gá»i LLM Ä‘á»ƒ phÃ¢n tÃ­ch


    return response.strip()


def propose_best_approach(conversation_goal, customer_info, product_info):
    """
    Sá»­ dá»¥ng LLM Ä‘á»ƒ suy luáº­n Best Approach phÃ¹ há»£p dá»±a trÃªn conversation_goal, customer_info vÃ  product_info.
    """

    print("customer_info in propose_best_approach:", customer_info)

    prompt = f"""
    ğŸ† Má»¥c tiÃªu há»™i thoáº¡i: "{conversation_goal}"
    ğŸ‘¤ ThÃ´ng tin khÃ¡ch hÃ ng: {json.dumps(customer_info, ensure_ascii=False)}
    ğŸ“¦ ThÃ´ng tin sáº£n pháº©m: {json.dumps(product_info, ensure_ascii=False)}

    âœ… HÃ£y táº¡o má»™t hÆ°á»›ng dáº«n (Best Approach) giÃºp nhÃ¢n viÃªn bÃ¡n hÃ ng nÃ³i chuyá»‡n há»£p lÃ½ vá»›i khÃ¡ch.
    âœ… Best Approach khÃ´ng pháº£i lÃ  cÃ¢u tráº£ lá»i trá»±c tiáº¿p, mÃ  lÃ  cÃ¡ch tiáº¿p cáº­n tá»•ng quan giÃºp cuá»™c trÃ² chuyá»‡n hiá»‡u quáº£ hÆ¡n.

    ğŸ”¹ Tráº£ lá»i CHá»ˆ DÆ¯á»šI Äá»ŠNH Dáº NG JSON nhÆ° sau:
    ```json
    {{ "best_approach": "<HÆ°á»›ng dáº«n ngáº¯n gá»n, sÃºc tÃ­ch, tá»‘i Ä‘a 2 cÃ¢u>" }}
    ```
    ğŸš« KhÃ´ng thÃªm báº¥t ká»³ vÄƒn báº£n nÃ o bÃªn ngoÃ i JSON.
    """

    response = llm.invoke(prompt).content  # Gá»i LLM

    try:
        # ğŸ’¡ Fix: Clean and parse JSON response
        json_str = response.strip().strip("```json").strip("```").strip()
        best_approach_data = json.loads(json_str)  # Parse cleaned JSON

        if "best_approach" not in best_approach_data:
            raise ValueError("Missing 'best_approach' in response")

        return best_approach_data["best_approach"]

    except Exception as e:
        print(f"âš ï¸ Error parsing best_approach: {e}, raw response: {response}")
        return "HÃ£y táº¡o sá»± tin tÆ°á»Ÿng vÃ  khuyáº¿n khÃ­ch khÃ¡ch hÃ ng."  # Fallback approach
import json
import re
import json
import re

def analyse_approach(customer_stage,conversation_goal, customer_info, product_info):
    """
    Sá»­ dá»¥ng LLM Ä‘á»ƒ suy luáº­n chiáº¿n thuáº­t tiáº¿p cáº­n khÃ¡ch hÃ ng vÃ  táº¡o hÆ°á»›ng dáº«n cho response prompt.
    
    ğŸ“Œ Output gá»“m:
    - best_approach: CÃ¡ch tiáº¿p cáº­n ngáº¯n gá»n Ä‘á»ƒ Ä‘áº¡t conversation_goal.
    - instruction: HÆ°á»›ng dáº«n cá»¥ thá»ƒ Ä‘á»ƒ truyá»n vÃ o response prompt.
    """

    print("customer_info in analyse_approach:", customer_info)

    prompt = f"""
    ğŸ† Má»¥c tiÃªu há»™i thoáº¡i: "{conversation_goal}"
    ğŸ“Œ Giai Ä‘oáº¡n khÃ¡ch hÃ ng: "{customer_stage}"
    ğŸ‘¤ ThÃ´ng tin khÃ¡ch hÃ ng: {json.dumps(customer_info, ensure_ascii=False)}
    ğŸ“¦ ThÃ´ng tin sáº£n pháº©m: {json.dumps(product_info, ensure_ascii=False)}

    âœ… HÃ£y phÃ¢n tÃ­ch hiá»‡n tráº¡ng khÃ¡ch hÃ ng vÃ  Ä‘á» xuáº¥t cÃ¡ch tiáº¿p cáº­n hiá»‡u quáº£ Ä‘á»ƒ Ä‘áº¡t má»¥c tiÃªu há»™i thoáº¡i.
    âœ… Sau Ä‘Ã³, táº¡o hÆ°á»›ng dáº«n (instruction) giÃºp AI sinh ra pháº£n há»“i há»£p lÃ½ trong cuá»™c trÃ² chuyá»‡n.

    ğŸ”¹ Tráº£ lá»i CHá»ˆ DÆ¯á»šI Äá»ŠNH Dáº NG JSON nhÆ° sau:
    ```json
    {{
        "best_approach": "<HÆ°á»›ng dáº«n tiáº¿p cáº­n ngáº¯n gá»n, tá»‘i Ä‘a 2 cÃ¢u>",
        "instruction": "<HÆ°á»›ng dáº«n chi tiáº¿t Ä‘á»ƒ truyá»n vÃ o response prompt>"
    }}
    ```
    ğŸš« KhÃ´ng thÃªm báº¥t ká»³ vÄƒn báº£n nÃ o bÃªn ngoÃ i JSON.
    """

    response = llm.invoke(prompt)

    if not response or not response.content:
        print("âš ï¸ LLM response is empty or None")
        return {
            "best_approach": "HÃ£y táº¡o sá»± tin tÆ°á»Ÿng vÃ  khuyáº¿n khÃ­ch khÃ¡ch hÃ ng.",
            "instruction": "HÃ£y pháº£n há»“i lá»‹ch sá»±, táº¡o sá»± tin tÆ°á»Ÿng vÃ  cung cáº¥p thÃªm thÃ´ng tin há»¯u Ã­ch."
        }

    raw_response = response.content.strip()
    
    # ğŸ’¡ Sá»­ dá»¥ng regex Ä‘á»ƒ láº¥y JSON chÃ­nh xÃ¡c (phÃ²ng khi LLM tráº£ vá» text láº«n JSON)
    match = re.search(r'\{.*\}', raw_response, re.DOTALL)

    if not match:
        print(f"âš ï¸ No valid JSON found in response: {raw_response}")
        return {
            "best_approach": "HÃ£y táº¡o sá»± tin tÆ°á»Ÿng vÃ  khuyáº¿n khÃ­ch khÃ¡ch hÃ ng.",
            "instruction": "HÃ£y pháº£n há»“i lá»‹ch sá»±, táº¡o sá»± tin tÆ°á»Ÿng vÃ  cung cáº¥p thÃªm thÃ´ng tin há»¯u Ã­ch."
        }

    json_str = match.group(0)

    try:
        result = json.loads(json_str)  # Parse JSON

        if "best_approach" not in result or "instruction" not in result:
            raise ValueError("Missing keys in JSON response")

        return result

    except Exception as e:
        print(f"âš ï¸ JSON parsing error: {e}, raw response: {json_str}")
        return {
            "best_approach": "HÃ£y táº¡o sá»± tin tÆ°á»Ÿng vÃ  khuyáº¿n khÃ­ch khÃ¡ch hÃ ng.",
            "instruction": "HÃ£y pháº£n há»“i lá»‹ch sá»±, táº¡o sá»± tin tÆ°á»Ÿng vÃ  cung cáº¥p thÃªm thÃ´ng tin há»¯u Ã­ch."
        }

def search_sales_skills(query_text, max_skills=3):
    """ 
    Truy váº¥n ká»¹ nÄƒng tá»« Pinecone vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n. 
    """
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-large", dimensions=1536)
    query_embedding = embedding_model.embed_query(query_text)

    index = pc.Index(index_name)
    response = index.query(
        vector=query_embedding,
        top_k=max_skills,
        include_metadata=True  
    )

    skills = []
    if response and "matches" in response:
        for match in response["matches"]:
            skill_text = match.get("metadata", {}).get("content")
            if skill_text:
                skills.append(skill_text)

    return skills if skills else ["KhÃ´ng tÃ¬m tháº¥y ká»¹ nÄƒng phÃ¹ há»£p."]



chain = (
    RunnablePassthrough.assign(
        history=lambda _: memory.load_memory_variables({}).get("history", []),
        products=lambda x: retrieve_product(x["user_input"]),
        sales_skills=lambda x: ", ".join(search_sales_skills(x["user_input"], max_skills=3)),  # Láº¥y ká»¹ nÄƒng tá»« Pinecone
        user_style=lambda _: "lá»‹ch sá»±"
    )  
    | prompt
    | llm
)

def ami_selling(user_message):
    """
    HÃ m chÃ­nh xá»­ lÃ½ há»™i thoáº¡i bÃ¡n hÃ ng cá»§a AMI.
    """
    global memory  # Ensure we are using the global memory instance

    # Save the user message to memory
    memory.save_context({"input": user_message}, {"output": ""})

    # Load the entire chat history
    chat_history_list = memory.load_memory_variables({})["history"]

    
    # Extract customer information from the chat history
    extracted_info = get_customer_info()
    
    # Update memory with extracted customer information (if needed)
    if extracted_info:
        extracted_info_str = json.dumps(extracted_info)
        memory.save_context({"input": "customer_info"}, {"output": extracted_info_str})
        user_context["customer_info"] = extracted_info
    else:
        memory.save_context({"input": "customer_info"}, {"output": "No customer information extracted."})

    print("Updated user_context:", user_context)

    company_goal = "KhÃ¡ch chuyá»ƒn khoáº£n"
    product_info = retrieve_product(user_message)

    # Gá»i handle_user_message Ä‘á»ƒ láº¥y pháº£n há»“i chÃ­nh theo Best_map
    response = ami_drive(user_message, user_context, company_goal, product_info)

    return response

