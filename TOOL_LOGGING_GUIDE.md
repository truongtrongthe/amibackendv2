# ğŸ”§ Tool Call Logging System

## Overview

The comprehensive tool call logging system provides **real-time visibility** into all tool executions with detailed console logging and streaming events for both server-side debugging and frontend user experience.

## ğŸ¯ Key Features

### âœ… **Dual Logging Approach**
- **Console Logging**: Detailed server-side logs with timestamps
- **Streaming Events**: Real-time frontend updates with tool status

### âœ… **Comprehensive Coverage**  
- **Before Execution**: Tool name, parameters, start time
- **During Execution**: Progress updates and status
- **After Execution**: Results, execution time, success/error status
- **Summary**: Total tools, success rate, total execution time

### âœ… **Multi-Provider Support**
- **Anthropic Tools**: Full logging in `anthropic_tool.py`
- **OpenAI Tools**: Full logging in `openai_tool.py`
- **Executive Tool**: Orchestration-level logging in `exec_tool.py`

## ğŸ“‹ Console Logging Format

### **Log Structure**
```
ğŸ”§ [PROVIDER_TOOL] HH:MM:SS - MESSAGE
```

### **Example Console Output**
```
ğŸ”§ [ANTHROPIC_TOOL] 14:23:15 - ğŸš€ Starting tool execution for query: 'What is AI in 2024?'
ğŸ”§ [ANTHROPIC_TOOL] 14:23:15 - ğŸ“‹ Available tools: 3 tools
ğŸ”§ [ANTHROPIC_TOOL] 14:23:15 - ğŸ” EXECUTING: search_google
ğŸ”§ [ANTHROPIC_TOOL] 14:23:15 -    Parameters: {'query': 'What is AI in 2024?'}
ğŸ”§ [ANTHROPIC_TOOL] 14:23:17 - âœ… SUCCESS: search_google completed in 1.85s
ğŸ”§ [ANTHROPIC_TOOL] 14:23:17 -    Result preview: AI in 2024 has seen remarkable advances...
ğŸ”§ [ANTHROPIC_TOOL] 14:23:18 - ğŸ“š EXECUTING: search_learning_context  
ğŸ”§ [ANTHROPIC_TOOL] 14:23:18 -    Parameters: {'query': 'What is AI in 2024?'}
ğŸ”§ [ANTHROPIC_TOOL] 14:23:19 - âœ… SUCCESS: search_learning_context completed in 0.95s
ğŸ”§ [ANTHROPIC_TOOL] 14:23:19 - ğŸ“Š TOOL EXECUTION SUMMARY:
ğŸ”§ [ANTHROPIC_TOOL] 14:23:19 -    Total tools: 2
ğŸ”§ [ANTHROPIC_TOOL] 14:23:19 -    Successful: 2
ğŸ”§ [ANTHROPIC_TOOL] 14:23:19 -    Failed: 0
ğŸ”§ [ANTHROPIC_TOOL] 14:23:19 -    Total time: 2.80s
```

## ğŸ“¡ Streaming Events

### **Event Types**

#### **1. Tool Execution Event**
```json
{
  "type": "tool_execution",
  "content": "ğŸ” Search completed (1.9s) - Found 2847 chars of results",
  "tool_name": "search_google", 
  "status": "completed",
  "execution_time": 1.85
}
```

#### **2. Tools Summary Event**
```json
{
  "type": "tools_summary",
  "content": "ğŸ Tools completed: 2/2 successful (2.8s total)",
  "tools_executed": [
    {
      "name": "search_google",
      "status": "success",
      "execution_time": 1.85,
      "result_length": 2847
    },
    {
      "name": "search_learning_context", 
      "status": "success",
      "execution_time": 0.95,
      "result_length": 1243
    }
  ],
  "total_execution_time": 2.80
}
```

#### **3. Error Event**
```json
{
  "type": "tool_execution",
  "content": "âŒ Search failed: API key invalid",
  "tool_name": "search_google",
  "status": "error", 
  "execution_time": 0.12
}
```

## ğŸ› ï¸ Implementation Details

### **Files Modified**

#### **1. `exec_tool.py`**
- Added `tool_logger` configuration
- Enhanced console logging format
- Added execution time tracking

#### **2. `anthropic_tool.py`**
- Comprehensive logging in `_stream_tool_execution`
- Before/during/after execution logging
- Error handling with detailed context
- Summary statistics generation

#### **3. `openai_tool.py`**
- Tool call logging in `_handle_tool_calls` 
- Streaming tool execution logging
- Parameter and result tracking
- Error categorization

### **Logger Configuration**
```python
# Configure detailed logging for tool calls
tool_logger = logging.getLogger("tool_calls")
tool_logger.setLevel(logging.INFO)
if not tool_logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('ğŸ”§ [PROVIDER_TOOL] %(asctime)s - %(message)s', datefmt='%H:%M:%S')
    handler.setFormatter(formatter)
    tool_logger.addHandler(handler)
```

## ğŸ“Š Logged Information

### **For Each Tool Execution**

#### **Before Execution**
- âœ… Tool name and provider
- âœ… Input parameters (with truncation for readability)
- âœ… Start timestamp
- âœ… Available tools count

#### **During Execution**
- âœ… Real-time status updates
- âœ… Progress indicators for long-running tools
- âœ… Parameter validation results

#### **After Execution**
- âœ… Success/error status
- âœ… Execution time (precise to milliseconds)
- âœ… Result preview (first 200 chars)
- âœ… Result length statistics
- âœ… Error messages with context

#### **Summary Statistics**
- âœ… Total tools executed
- âœ… Success/failure counts
- âœ… Total execution time
- âœ… Performance metrics

## ğŸ§ª Testing

### **Run Comprehensive Tests**
```bash
python test_tool_logging.py
```

### **Test Scenarios Covered**
1. **Learning Requests**: Tools like `search_learning_context`, `analyze_learning_opportunity`
2. **Search Requests**: Web search with `search_google`
3. **Context Requests**: Context retrieval with `get_context` 
4. **Error Scenarios**: API failures, invalid parameters, tool not found
5. **Multi-tool Scenarios**: Multiple tools in sequence

### **Expected Output**
The test script demonstrates:
- Console logging for all tool executions
- Streaming events with detailed status
- Error handling and recovery
- Performance metrics and summaries

## ğŸ›ï¸ Configuration Options

### **Log Levels**
- `INFO`: Standard tool execution logging (default)
- `DEBUG`: Verbose parameter and result logging  
- `WARNING`: Only errors and warnings
- `ERROR`: Only critical failures

### **Streaming Control**
- Enable/disable tool execution events
- Configure result preview length
- Control summary frequency

### **Console Output**
- Customize log format and timestamps
- Filter by tool type or provider
- Adjust verbosity levels

## ğŸ“ˆ Benefits for Debugging

### **Server-Side Debugging**
- âœ… **Trace Tool Execution**: See exactly which tools are called and when
- âœ… **Performance Analysis**: Identify slow-performing tools
- âœ… **Error Diagnosis**: Get detailed context for failures
- âœ… **Parameter Validation**: Verify tool inputs are correct

### **Frontend Development**
- âœ… **Real-time Updates**: Show users what's happening
- âœ… **Progress Indicators**: Display tool execution progress  
- âœ… **Error Handling**: Provide meaningful error messages
- âœ… **Performance Feedback**: Show execution times to users

### **System Monitoring** 
- âœ… **Tool Usage Statistics**: Track which tools are used most
- âœ… **Performance Metrics**: Monitor average execution times
- âœ… **Error Rates**: Track tool failure rates
- âœ… **Resource Usage**: Understand tool resource consumption

## ğŸ” Example: Learning Tool Execution

### **User Request**
```
"CÃ´ng ty cá»§a tÃ´i cÃ³ 100 nhÃ¢n viÃªn vÃ  chÃºng tÃ´i Ä‘ang phÃ¡t triá»ƒn sáº£n pháº©m AI"
(My company has 100 employees and we're developing AI products)
```

### **Console Log Output**
```
ğŸ”§ [ANTHROPIC_TOOL] 14:30:15 - ğŸš€ Starting tool execution for query: 'CÃ´ng ty cá»§a tÃ´i cÃ³ 100 nhÃ¢n viÃªn...'
ğŸ”§ [ANTHROPIC_TOOL] 14:30:15 - ğŸ“‹ Available tools: 5 tools
ğŸ”§ [ANTHROPIC_TOOL] 14:30:15 - ğŸ“š EXECUTING: search_learning_context
ğŸ”§ [ANTHROPIC_TOOL] 14:30:15 -    Parameters: {'query': 'CÃ´ng ty cá»§a tÃ´i cÃ³ 100 nhÃ¢n viÃªn...'}
ğŸ”§ [ANTHROPIC_TOOL] 14:30:16 - âœ… SUCCESS: search_learning_context completed in 0.83s
ğŸ”§ [ANTHROPIC_TOOL] 14:30:16 -    Result preview: No similar knowledge found in database...
ğŸ”§ [ANTHROPIC_TOOL] 14:30:16 - ğŸ§  EXECUTING: analyze_learning_opportunity
ğŸ”§ [ANTHROPIC_TOOL] 14:30:16 -    Parameters: {'user_message': 'CÃ´ng ty cá»§a tÃ´i cÃ³ 100 nhÃ¢n viÃªn...'}
ğŸ”§ [ANTHROPIC_TOOL] 14:30:18 - âœ… SUCCESS: analyze_learning_opportunity completed in 1.45s
ğŸ”§ [ANTHROPIC_TOOL] 14:30:18 -    Result preview: SHOULD_LEARN: High-value company information detected...
ğŸ”§ [ANTHROPIC_TOOL] 14:30:18 - ğŸ¤ EXECUTING: request_learning_decision  
ğŸ”§ [ANTHROPIC_TOOL] 14:30:18 -    Reason: Learning analysis suggested saving content
ğŸ”§ [ANTHROPIC_TOOL] 14:30:19 - âœ… SUCCESS: request_learning_decision completed in 0.67s
ğŸ”§ [ANTHROPIC_TOOL] 14:30:19 -    Decision created for human approval
ğŸ”§ [ANTHROPIC_TOOL] 14:30:19 - ğŸ“Š TOOL EXECUTION SUMMARY:
ğŸ”§ [ANTHROPIC_TOOL] 14:30:19 -    Total tools: 3
ğŸ”§ [ANTHROPIC_TOOL] 14:30:19 -    Successful: 3
ğŸ”§ [ANTHROPIC_TOOL] 14:30:19 -    Failed: 0
ğŸ”§ [ANTHROPIC_TOOL] 14:30:19 -    Total time: 2.95s
```

### **Frontend Streaming Events**
```json
{"type": "tool_execution", "content": "ğŸ“š Learning context search completed (0.8s)", "tool_name": "search_learning_context", "status": "completed"}
{"type": "tool_execution", "content": "ğŸ§  Learning analysis completed (1.5s)", "tool_name": "analyze_learning_opportunity", "status": "completed"}  
{"type": "tool_execution", "content": "ğŸ¤ Learning decision created (0.7s) - Awaiting human approval", "tool_name": "request_learning_decision", "status": "completed"}
{"type": "tools_summary", "content": "ğŸ Tools completed: 3/3 successful (3.0s total)", "total_execution_time": 2.95}
```

## ğŸš€ Usage in Production

### **Enable Tool Logging**
Tool logging is **enabled by default** in the system. No additional configuration needed.

### **View Logs in Real-Time** 
```bash
# Watch server logs
tail -f app.log | grep "TOOL"

# Filter by tool type
tail -f app.log | grep "search_google"

# Monitor errors only  
tail -f app.log | grep "âŒ"
```

### **Frontend Integration**
Listen for tool execution events in your frontend:
```javascript
eventSource.addEventListener('message', (event) => {
  const data = JSON.parse(event.data);
  
  if (data.type === 'tool_execution') {
    console.log(`Tool ${data.tool_name}: ${data.status}`);
    updateProgressIndicator(data);
  }
  
  if (data.type === 'tools_summary') {
    console.log(`Tools completed: ${data.content}`);
    hideProgressIndicators();
  }
});
```

## ğŸ¯ Key Benefits Summary

### **ğŸ” Complete Visibility**
- See exactly when each tool is called
- Track all parameters and results
- Monitor execution times and performance
- Get detailed error context

### **ğŸ› Enhanced Debugging** 
- Trace tool execution flows
- Identify bottlenecks and failures
- Validate tool inputs and outputs
- Monitor system health

### **ğŸ‘¥ Better User Experience**
- Real-time progress updates
- Transparent tool execution
- Meaningful error messages
- Performance feedback

### **ğŸ“Š Data-Driven Insights**
- Tool usage statistics
- Performance metrics
- Error analysis
- Resource optimization

The tool call logging system transforms the `/tool/llm` endpoint from a "black box" into a **fully transparent, debuggable, and monitorable** system that provides clear visibility into every aspect of tool execution. 