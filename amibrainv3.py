from langchain_openai import ChatOpenAI
from knowledge import  retrieve_relevant_infov2 
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
import os
from langchain_openai import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec

# Initialize LLM for conversation
llm = ChatOpenAI(model="gpt-4o", streaming=True)

prompt = PromptTemplate(
    input_variables=["history", "user_input", "products", "user_style", "sales_skills"],
    template="""
    Dá»±a vÃ o cÃ¡c thÃ´ng tin trÆ°á»›c Ä‘Ã¢y cá»§a ngÆ°á»i dÃ¹ng, hÃ£y Ä‘áº£m báº£o cÃ¢u tráº£ lá»i phÃ¹ há»£p vá»›i phong cÃ¡ch cá»§a há».
    Lá»‹ch sá»­ cuá»™c trÃ² chuyá»‡n:
    {history}

    Sáº£n pháº©m liÃªn quan:
    {products}

    ğŸ“Œ Ká»¹ nÄƒng bÃ¡n hÃ ng cáº§n Ã¡p dá»¥ng:
    {sales_skills}

    ğŸ“¢ **HÆ°á»›ng dáº«n cho AMI:**
    1. Ãp dá»¥ng cÃ¡c ká»¹ nÄƒng vÃ o pháº£n há»“i Ä‘á»ƒ giÃºp cuá»™c trÃ² chuyá»‡n tá»± nhiÃªn hÆ¡n.
    2. Náº¿u cÃ³ ká»¹ nÄƒng "Láº¯ng nghe chá»§ Ä‘á»™ng" â†’ HÃ£y paraphrase láº¡i Ã½ cá»§a khÃ¡ch hÃ ng trÆ°á»›c khi tÆ° váº¥n.
    3. Náº¿u cÃ³ ká»¹ nÄƒng "Ká»ƒ chuyá»‡n" â†’ HÃ£y thÃªm má»™t cÃ¢u chuyá»‡n ngáº¯n Ä‘á»ƒ minh há»a sáº£n pháº©m.
    4. Náº¿u cÃ³ ká»¹ nÄƒng "Giáº£i quyáº¿t pháº£n Ä‘á»‘i" â†’ HÃ£y xá»­ lÃ½ lo ngáº¡i cá»§a khÃ¡ch hÃ ng trÆ°á»›c khi tÆ° váº¥n.

    NgÆ°á»i dÃ¹ng: {user_input}
    ğŸ¯ **AMI (giá»¯ nguyÃªn phong cÃ¡ch cá»§a ngÆ°á»i dÃ¹ng + Ã¡p dá»¥ng ká»¹ nÄƒng bÃ¡n hÃ ng):**
    """
)

#  chat_history = ChatMessageHistory()
chat_history = ChatMessageHistory()

memory = ConversationBufferMemory(
    chat_memory=chat_history,
    memory_key="history",  # REQUIRED in newer versions
    return_messages=True
)

def retrieve_product(user_input):
    """Retrieve relevant context from Pinecone and return a structured summary."""
    retrieved_info = retrieve_relevant_infov2(user_input, top_k=10)

    if not retrieved_info:
        return "KhÃ´ng tÃ¬m tháº¥y ngá»¯ cáº£nh phÃ¹ há»£p."

    structured_summary = []
    for doc in retrieved_info:
        content = doc.get("content", "").strip()
        if content:
            structured_summary.append(content)
    return "\n\n".join(structured_summary) if structured_summary else "KhÃ´ng tÃ¬m tháº¥y ngá»¯ cáº£nh phÃ¹ há»£p."


PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "")
PINECONE_ENV = "us-east-1"  # Check Pinecone console for your region


index_name = "ami-knowledge"
pc = Pinecone(api_key=PINECONE_API_KEY)

# Check if index exists
existing_indexes = [i['name'] for i in pc.list_indexes()]
if index_name not in existing_indexes:
    pc.create_index(
        name=index_name,
        dimension=1536,  # Ensure this matches your model's output dimension
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )


def search_sales_skills(query_text, max_skills=3):
    """Truy váº¥n ká»¹ nÄƒng bÃ¡n hÃ ng tá»« Pinecone dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá»›i input cá»§a ngÆ°á»i dÃ¹ng."""
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-large", dimensions=1536)
    query_embedding = embedding_model.embed_query(query_text)

    # TÃ¬m kiáº¿m trong Pinecone (dÃ¹ng cosine similarity)
    index = pc.Index(index_name)
    response = index.query(
        vector=query_embedding,
        top_k=max_skills,  # Sá»‘ lÆ°á»£ng ká»¹ nÄƒng tá»‘i Ä‘a
        include_metadata=True  # Láº¥y luÃ´n thÃ´ng tin ká»¹ nÄƒng tá»« metadata
    )

    skills = []
    if response and "matches" in response:
        for match in response["matches"]:
            skill_text = match.get("metadata", {}).get("skill_text")
            if skill_text:
                skills.append(skill_text)

    return skills if skills else ["KhÃ´ng tÃ¬m tháº¥y ká»¹ nÄƒng phÃ¹ há»£p."]


chain = (
    RunnablePassthrough.assign(
        history=lambda _: memory.load_memory_variables({}).get("history", []),
        products=lambda x: retrieve_product(x["user_input"]),
        sales_skills=lambda x: ", ".join(search_sales_skills(x["user_input"], max_skills=3)),  # Láº¥y ká»¹ nÄƒng tá»« Pinecone
        user_style=lambda _: "lá»‹ch sá»±"
    )  
    | prompt
    | llm
)

def ami_selling(query):
    input_data = {"user_input": query}
    print("Current Memory:", memory.load_memory_variables({}))
    
    last_response = ""

    # ğŸ”¥ DÃ¹ng yield from Ä‘á»ƒ Ä‘áº£m báº£o stream cháº£y Ä‘Ãºng
    response_stream = chain.stream(input_data)
    yield from (chunk.content for chunk in response_stream)  # âœ… CÃ¡ch khÃ¡c

    # âœ… LÆ°u vÃ o memory sau khi hoÃ n thÃ nh
    memory.save_context({"input": query}, {"output": last_response.strip()})
