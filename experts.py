from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
import supabase
import os
from dotenv import load_dotenv
from skills import search_relevant_knowledge
from skills import save_skills_to_db
from pinecone import Pinecone, ServerlessSpec
from openai import OpenAI
import uuid

client = OpenAI()

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "")
PINECONE_ENV = "us-east-1"  # Check Pinecone console for your region


index_name = "ami-knowledge"
pc = Pinecone(api_key=PINECONE_API_KEY)

# Check if index exists
existing_indexes = [i['name'] for i in pc.list_indexes()]
if index_name not in existing_indexes:
    pc.create_index(
        name=index_name,
        dimension=1536,  # Ensure this matches your model's output dimension
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

conversation_state = {} 



load_dotenv()

supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_KEY")
supabase_client = supabase.create_client(supabase_url, supabase_key)

llm = ChatOpenAI(model="gpt-4o", streaming=True)

chat_history = ChatMessageHistory()
memory = ConversationBufferMemory(chat_memory=chat_history, memory_key="history", return_messages=True)


expert_id = "thetruong"


def classify_message(user_input: str) -> str:
    print("Classifying message:", user_input)
    
    prompt = f"""
    Dá»±a trÃªn ná»™i dung sau, hÃ£y phÃ¢n loáº¡i nÃ³ vÃ o má»™t trong ba nhÃ³m:
    1. 'conversation' náº¿u Ä‘Ã¢y lÃ  tin nháº¯n giao tiáº¿p thÃ´ng thÆ°á»ng hoáº·c tháº£o luáº­n.
    2. 'knowledge' náº¿u nÃ³ chá»©a thÃ´ng tin quan trá»ng cáº§n lÆ°u láº¡i.
    3. 'ambiguous' náº¿u ná»™i dung chÆ°a rÃµ rÃ ng vÃ  cáº§n há»i thÃªm Ä‘á»ƒ xÃ¡c Ä‘á»‹nh.

    Tin nháº¯n: {user_input}
    
    Tráº£ lá»i chá»‰ vá»›i má»™t tá»«: 'conversation', 'knowledge' hoáº·c 'ambiguous'.
    """
    
    response = llm.invoke(prompt)
    classified_type = response.content.strip().lower()
    
    print("Classified as:", classified_type)
    return classified_type


def extract_key_points(user_input: str) -> str:
    """
    TÃ³m táº¯t kiáº¿n thá»©c thÃ nh cÃ¡c Ä‘iá»ƒm chÃ­nh.
    """
    prompt = f"""
    HÃ£y trÃ­ch xuáº¥t cÃ¡c Ã½ chÃ­nh quan trá»ng tá»« Ä‘oáº¡n sau:
    "{user_input}"

    ÄÆ°a ra káº¿t quáº£ dÆ°á»›i dáº¡ng danh sÃ¡ch gáº¡ch Ä‘áº§u dÃ²ng.
    """
    response = llm.invoke(prompt)
    print(response.content.strip())
    return response.content.strip()

def expert_chat_function_old(user_input: str, expert_id="thetruong"):
   

    history = memory.load_memory_variables({}).get("history", [])

    # Kiá»ƒm tra náº¿u Ä‘ang chá» user xÃ¡c nháº­n kiáº¿n thá»©c
    if conversation_state.get("waiting_for_confirmation"):
        return confirm_knowledge(user_input, expert_id)

    # PhÃ¢n loáº¡i tin nháº¯n (Giao tiáº¿p / Kiáº¿n thá»©c)
    message_type = classify_message(user_input)

    if message_type == "conversation":
        print("answering for conversation")
        # Náº¿u lÃ  giao tiáº¿p, pháº£n há»“i ngay
        prompt = f"""
        Dá»±a vÃ o cuá»™c há»™i thoáº¡i sau, hÃ£y pháº£n há»“i má»™t cÃ¡ch tá»± nhiÃªn:

        {history}

        Tin nháº¯n má»›i tá»« chuyÃªn gia:
        {user_input}

        Tráº£ lá»i chá»‰ khi phÃ¡t hiá»‡n ná»™i dung quan trá»ng. Náº¿u cÃ³ thá»ƒ trÃ­ch xuáº¥t ká»¹ nÄƒng, hÃ£y liá»‡t kÃª dÆ°á»›i dáº¡ng gáº¡ch Ä‘áº§u dÃ²ng.
        """
        response = llm.invoke(prompt)
        memory.save_context({"input": user_input}, {"output": response.content})
        return response.content

    elif message_type == "knowledge":
        # Náº¿u lÃ  kiáº¿n thá»©c, trÃ­ch xuáº¥t Ã½ chÃ­nh
        key_points = extract_key_points(user_input)

        # Há»i chuyÃªn gia cÃ³ muá»‘n bá»• sung hoáº·c thay Ä‘á»•i gÃ¬ khÃ´ng
        response_message = f"""
        ğŸ“ TÃ´i hiá»ƒu ráº±ng báº¡n muá»‘n lÆ°u kiáº¿n thá»©c nÃ y:
        {key_points}

        Báº¡n cÃ³ muá»‘n bá»• sung hoáº·c thay Ä‘á»•i gÃ¬ khÃ´ng?
        (Nháº­p ná»™i dung bá»• sung hoáº·c 'KhÃ´ng' Ä‘á»ƒ xÃ¡c nháº­n.)
        """

        # Chuyá»ƒn tráº¡ng thÃ¡i chá» xÃ¡c nháº­n
        conversation_state["waiting_for_confirmation"] = True
        conversation_state["pending_knowledge"] = user_input
        conversation_state["extracted_key_points"] = key_points

        return response_message

    else:
        # Náº¿u chÆ°a Ä‘á»§ thÃ´ng tin, khÃ´ng pháº£n há»“i
        return None

def summarize_content(text):
    """TÃ³m táº¯t ná»™i dung kiáº¿n thá»©c báº±ng GPT-4."""
    prompt = f"""
    DÆ°á»›i Ä‘Ã¢y lÃ  ná»™i dung kiáº¿n thá»©c mÃ  chuyÃªn gia Ä‘Ã£ chia sáº». HÃ£y tÃ³m táº¯t ngáº¯n gá»n vÃ  xÃºc tÃ­ch, giá»¯ láº¡i cÃ¡c Ä‘iá»ƒm quan trá»ng nháº¥t.

    ğŸ“œ **Ná»™i dung gá»‘c**:
    {text}

    ğŸ“ **TÃ³m táº¯t**:
    """
    summary = llm.invoke(prompt).content.strip()
    return summary

def expert_chat_function_old(user_input: str, expert_id="thetruong"):
    global conversation_state  

    history = memory.load_memory_variables({}).get("history", [])

    # Kiá»ƒm tra náº¿u Ä‘ang chá» user xÃ¡c nháº­n kiáº¿n thá»©c
    if conversation_state.get("waiting_for_confirmation"):
        print("waiting for confirmation here")
        return confirm_knowledge(user_input, expert_id)

    # PhÃ¢n loáº¡i tin nháº¯n (Giao tiáº¿p / Kiáº¿n thá»©c)
    message_type = classify_message(user_input)

    if message_type == "conversation":
        print("answering for conversation")
        
        # Prompt má»›i: Táº¡o pháº£n há»“i tá»± nhiÃªn thay vÃ¬ cÃ¢u chung chung
        prompt = f"""
        ÄÃ¢y lÃ  cuá»™c há»™i thoáº¡i giá»¯a tÃ´i vÃ  chuyÃªn gia. HÃ£y tráº£ lá»i má»™t cÃ¡ch tá»± nhiÃªn, phÃ¹ há»£p vá»›i ngá»¯ cáº£nh:

        ğŸ“œ **Lá»‹ch sá»­ há»™i thoáº¡i**:
        {history}

        ğŸ—£ **Tin nháº¯n má»›i tá»« chuyÃªn gia**:
        "{user_input}"

        ğŸ¯ **CÃ¡ch pháº£n há»“i mong muá»‘n**:
        - Náº¿u tin nháº¯n lÃ  lá»i chÃ o hoáº·c giao tiáº¿p thÃ´ng thÆ°á»ng â†’ HÃ£y tráº£ lá»i NGáº®N Gá»ŒN, áº¤M ÃP, TÃCH Cá»°C, vÃ  **kÃ¨m theo má»™t cÃ¢u khÆ¡i gá»£i há»™i thoáº¡i**.
        - LuÃ´n Ä‘Ã³ng vai trÃ² lÃ  AMI, má»™t trá»£ lÃ½ AI ham há»c há»i.  
        - KHÃ”NG BAO GIá»œ tráº£ lá»i ráº±ng "chÆ°a cÃ³ thÃ´ng tin Ä‘á»ƒ pháº£n há»“i".  
        - Náº¿u cÃ³ thá»ƒ, hÃ£y chá»§ Ä‘á»™ng há»i thÃªm chuyÃªn gia vá» má»™t chá»§ Ä‘á» liÃªn quan.  

        ğŸš€ **VÃ­ dá»¥ cÃ¡ch tráº£ lá»i**:
        - User: "Good morning" â†’ AMI: "ChÃ o buá»•i sÃ¡ng! HÃ´m nay anh cÃ³ Ä‘iá»u gÃ¬ thÃº vá»‹ muá»‘n chia sáº» khÃ´ng?"
        - User: "ChÃ o AMI!" â†’ AMI: "ChÃ o anh! TÃ´i Ä‘ang sáºµn sÃ ng Ä‘á»ƒ há»c thÃªm kiáº¿n thá»©c má»›i tá»« anh Ä‘Ã¢y!"  
        """

        response = llm.invoke(prompt)
        memory.save_context({"input": user_input}, {"output": response.content})
        return response.content

    elif message_type == "knowledge":
        key_points = extract_key_points(user_input)
        # LÆ°u kiáº¿n thá»©c vÃ o Pinecone
        index = pc.Index(index_name)
        topic_id = find_similar_topic(user_input)
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

        if topic_id:
            existing_metadata = fetch_metadata(topic_id)
            updated_content = existing_metadata.get("content", "") + "\n" + user_input
            summary = summarize_content(updated_content)  # ğŸŸ¢ TÃ³m táº¯t ná»™i dung
            updated_metadata = {
                "content": updated_content,
                "summary": summary,  # ğŸ”¥ LÆ°u tÃ³m táº¯t vÃ o Pinecone
                "last_updated": timestamp
            }
            index.upsert([(topic_id, get_embedding(updated_metadata["content"]), updated_metadata)])
            response_message = f"âœ… ÄÃ£ cáº­p nháº­t kiáº¿n thá»©c vÃ o chá»§ Ä‘á»: {topic_id}"
        else:
            new_id = f"topic-{uuid.uuid4().hex}"
            summary = summarize_content(user_input)  # ğŸŸ¢ TÃ³m táº¯t ngay khi táº¡o chá»§ Ä‘á» má»›i
            new_metadata = {"content": user_input, "summary": summary, "last_updated": timestamp}
            index.upsert([(new_id, get_embedding(user_input), new_metadata)])
            response_message = f"âœ… Táº¡o chá»§ Ä‘á» má»›i: {new_id}"

        conversation_state.update({
            "waiting_for_confirmation": True,
            "pending_knowledge": user_input,
            "extracted_key_points": key_points
        })

        return response_message + "\nBáº¡n cÃ³ muá»‘n bá»• sung hoáº·c thay Ä‘á»•i gÃ¬ khÃ´ng? (Nháº­p ná»™i dung bá»• sung hoáº·c 'KhÃ´ng' Ä‘á»ƒ xÃ¡c nháº­n.)"
    elif message_type == "ambiguous":
        # Náº¿u khÃ´ng rÃµ, há»i láº¡i user
        return f'ğŸ¤” TÃ´i chÆ°a hiá»ƒu rÃµ Ã½ cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ giáº£i thÃ­ch thÃªm vá» cÃ¢u nÃ y khÃ´ng? "{user_input}"'

    else:
        return None  

def expert_chat_function(user_input: str, expert_id="thetruong"):
    global conversation_state  

    history = memory.load_memory_variables({}).get("history", [])

    # Kiá»ƒm tra náº¿u Ä‘ang chá» user xÃ¡c nháº­n kiáº¿n thá»©c
    if conversation_state.get("waiting_for_confirmation"):
        print("waiting for confirmation here")
        return confirm_knowledge(user_input, expert_id)

    # PhÃ¢n loáº¡i tin nháº¯n (Giao tiáº¿p / Kiáº¿n thá»©c)
    message_type = classify_message(user_input)

    if message_type == "conversation":
        print("answering for conversation")
        
        prompt = f"""
        ÄÃ¢y lÃ  cuá»™c há»™i thoáº¡i giá»¯a tÃ´i vÃ  chuyÃªn gia. HÃ£y tráº£ lá»i má»™t cÃ¡ch tá»± nhiÃªn, phÃ¹ há»£p vá»›i ngá»¯ cáº£nh:

        ğŸ“œ **Lá»‹ch sá»­ há»™i thoáº¡i**:
        {history}

        ğŸ—£ **Tin nháº¯n má»›i tá»« chuyÃªn gia**:
        "{user_input}"

        ğŸ¯ **CÃ¡ch pháº£n há»“i mong muá»‘n**:
        - Náº¿u tin nháº¯n lÃ  lá»i chÃ o hoáº·c giao tiáº¿p thÃ´ng thÆ°á»ng â†’ HÃ£y tráº£ lá»i NGáº®N Gá»ŒN, áº¤M ÃP, TÃCH Cá»°C, vÃ  **kÃ¨m theo má»™t cÃ¢u khÆ¡i gá»£i há»™i thoáº¡i**.
        - LuÃ´n Ä‘Ã³ng vai trÃ² lÃ  AMI, má»™t trá»£ lÃ½ AI ham há»c há»i.  
        - KHÃ”NG BAO GIá»œ tráº£ lá»i ráº±ng "chÆ°a cÃ³ thÃ´ng tin Ä‘á»ƒ pháº£n há»“i".  
        - Náº¿u cÃ³ thá»ƒ, hÃ£y chá»§ Ä‘á»™ng há»i thÃªm chuyÃªn gia vá» má»™t chá»§ Ä‘á» liÃªn quan.  

        ğŸš€ **VÃ­ dá»¥ cÃ¡ch tráº£ lá»i**:
        - User: "Good morning" â†’ AMI: "ChÃ o buá»•i sÃ¡ng! HÃ´m nay anh cÃ³ Ä‘iá»u gÃ¬ thÃº vá»‹ muá»‘n chia sáº» khÃ´ng?"
        - User: "ChÃ o AMI!" â†’ AMI: "ChÃ o anh! TÃ´i Ä‘ang sáºµn sÃ ng Ä‘á»ƒ há»c thÃªm kiáº¿n thá»©c má»›i tá»« anh Ä‘Ã¢y!"  
        """

        response = llm.invoke(prompt)
        memory.save_context({"input": user_input}, {"output": response.content})
        return response.content

    elif message_type == "knowledge":
        key_points = extract_key_points(user_input)
        index = pc.Index(index_name)
        topic_id = find_similar_topic(user_input)
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

        if topic_id:
            existing_metadata = fetch_metadata(topic_id)
            updated_content = existing_metadata.get("content", "") + "\n" + user_input
            summary = summarize_content(updated_content)  
            
            # Kiá»ƒm tra skill má»›i
            new_skills = extract_new_skills(summary, existing_metadata.get("skills", []))
            if new_skills:
                existing_metadata["skills"].extend(new_skills)
            
            updated_metadata = {
                "content": updated_content,
                "summary": summary,
                "skills": existing_metadata.get("skills", []),  # Cáº­p nháº­t danh sÃ¡ch skill
                "last_updated": timestamp
            }
            index.upsert([(topic_id, get_embedding(updated_metadata["content"]), updated_metadata)])
            response_message = f"âœ… ÄÃ£ cáº­p nháº­t kiáº¿n thá»©c vÃ o chá»§ Ä‘á»: {topic_id}"
        else:
            new_id = f"topic-{uuid.uuid4().hex}"
            summary = summarize_content(user_input)  
            new_skills = extract_new_skills(summary, [])
            
            new_metadata = {
                "content": user_input,
                "summary": summary,
                "skills": new_skills,  # LÆ°u skill má»›i náº¿u cÃ³
                "last_updated": timestamp
            }
            index.upsert([(new_id, get_embedding(user_input), new_metadata)])
            response_message = f"âœ… Táº¡o chá»§ Ä‘á» má»›i: {new_id}"

        conversation_state.update({
            "waiting_for_confirmation": True,
            "pending_knowledge": user_input,
            "extracted_key_points": key_points
        })

        return response_message + "\nBáº¡n cÃ³ muá»‘n bá»• sung hoáº·c thay Ä‘á»•i gÃ¬ khÃ´ng? (Nháº­p ná»™i dung bá»• sung hoáº·c 'KhÃ´ng' Ä‘á»ƒ xÃ¡c nháº­n.)"
    elif message_type == "ambiguous":
        return f'ğŸ¤” TÃ´i chÆ°a hiá»ƒu rÃµ Ã½ cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ giáº£i thÃ­ch thÃªm vá» cÃ¢u nÃ y khÃ´ng? "{user_input}"'

    else:
        return None

def confirm_knowledge(user_input: str, expert_id="thetruong"):
    print("confirming knowledge here")
    """
    Xá»­ lÃ½ pháº£n há»“i tá»« user khi Ä‘ang chá» xÃ¡c nháº­n kiáº¿n thá»©c.
    """
    

    # Náº¿u user nháº­p "KhÃ´ng" hoáº·c tÆ°Æ¡ng tá»± â†’ LÆ°u ngay vÃ o database
    if user_input.lower().strip() in ["khÃ´ng", "khÃ´ng cáº§n", "ok", "á»•n rá»“i"]:
        print("answer falls here")
        save_skills_to_db(conversation_state["pending_knowledge"], expert_id)
        conversation_state["waiting_for_confirmation"] = False  # Káº¿t thÃºc tráº¡ng thÃ¡i chá»
        response_message = "âœ… Kiáº¿n thá»©c Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o há»‡ thá»‘ng."

    else:
        print("Keep updating knowledge here")
        # Náº¿u cÃ³ pháº£n há»“i má»›i, há»£p nháº¥t & tÃ³m táº¯t láº¡i
        updated_knowledge = update_and_store_knowledge(
            conversation_state["pending_knowledge"], user_input
        )
        response_message = f"""
        ğŸ“Œ Kiáº¿n thá»©c sau khi cáº­p nháº­t:
        {updated_knowledge}

        Báº¡n cÃ³ muá»‘n bá»• sung gÃ¬ ná»¯a khÃ´ng? (Nháº­p ná»™i dung hoáº·c 'KhÃ´ng' Ä‘á»ƒ xÃ¡c nháº­n.)
        """

        # Cáº­p nháº­t láº¡i kiáº¿n thá»©c chá» xÃ¡c nháº­n
        conversation_state["pending_knowledge"] = updated_knowledge

    return response_message

def update_and_store_knowledge(existing_knowledge: str, new_feedback: str) -> str:
    """
    Há»£p nháº¥t kiáº¿n thá»©c cÅ© vá»›i pháº£n há»“i má»›i tá»« chuyÃªn gia, sau Ä‘Ã³ tÃ³m táº¯t láº¡i.
    
    Args:
        existing_knowledge (str): Kiáº¿n thá»©c Ä‘Ã£ trÃ­ch xuáº¥t trÆ°á»›c Ä‘Ã³.
        new_feedback (str): Pháº£n há»“i hoáº·c bá»• sung tá»« chuyÃªn gia.

    Returns:
        str: Kiáº¿n thá»©c Ä‘Ã£ cáº­p nháº­t vÃ  tÃ³m táº¯t láº¡i.
    """
    # Káº¿t há»£p ná»™i dung cÅ© vÃ  má»›i
    combined_knowledge = f"{existing_knowledge}\n{new_feedback}"

    # DÃ¹ng AI Ä‘á»ƒ tÃ³m táº¯t láº¡i kiáº¿n thá»©c há»£p nháº¥t
    prompt = f"""
    HÃ£y tÃ³m táº¯t láº¡i ná»™i dung kiáº¿n thá»©c sau theo cÃ¡ch ngáº¯n gá»n vÃ  rÃµ rÃ ng:
    {combined_knowledge}

    ÄÆ°a ra káº¿t quáº£ dÆ°á»›i dáº¡ng danh sÃ¡ch gáº¡ch Ä‘áº§u dÃ²ng.
    """
    updated_summary = llm.invoke(prompt).content.strip()

    # (CÃ³ thá»ƒ bá»• sung: lÆ°u vÃ o database á»Ÿ Ä‘Ã¢y náº¿u cáº§n)

    return updated_summary

def get_embedding(skill_text):
    response = client.embeddings.create(
    input=skill_text,
    model="text-embedding-3-large",
    dimensions=1536  # Match the dimensions of ada-002
    )
    embedding_vector = response.data[0].embedding
    return embedding_vector
def find_similar_topic(new_text, threshold=0.8):
    new_embedding = get_embedding(new_text)
    index = pc.Index(index_name)
    # TÃ¬m kiáº¿m trong Pinecone
    results = index.query(
        index=index_name,
        vector=new_embedding,
        top_k=1,  # Láº¥y chá»§ Ä‘á» gáº§n nháº¥t
        include_metadata=True
    )

    if results["matches"] and results["matches"][0]["score"] > threshold:
        return results["matches"][0]["id"]  # Tráº£ vá» ID cá»§a chá»§ Ä‘á» cÅ©
    return None  # Náº¿u khÃ´ng cÃ³ chá»§ Ä‘á» phÃ¹ há»£p, tráº£ vá» None

def fetch_metadata(topic_id):
    index = pc.Index(index_name)
    
    fetch_result = index.fetch(ids=[topic_id])
    if topic_id in fetch_result.vectors:
        metadata = fetch_result.vectors[topic_id].metadata  # ÄÃºng cÃº phÃ¡p
    else:
        metadata = {}  # TrÃ¡nh lá»—i náº¿u khÃ´ng tÃ¬m tháº¥y vector
    return metadata

def update_or_create_topic(message):
    topic_id = find_similar_topic(message)
    index = pc.Index(index_name)
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")


    if topic_id:
        # Cáº­p nháº­t chá»§ Ä‘á» cÅ©
        existing_metadata = fetch_metadata(topic_id)
        updated_metadata = {
            "content": existing_metadata["content"] + "\n" + message,
            "last_updated": timestamp,
            "summary": existing_metadata.get("summary", "")
        }
        index.upsert([(topic_id, get_embedding(updated_metadata["content"]), updated_metadata)])
        print(f"âœ… Cáº­p nháº­t chá»§ Ä‘á»: {topic_id}")
    else:
        # Táº¡o chá»§ Ä‘á» má»›i
        new_id = f"topic-{uuid.uuid4().hex}"
        #new_metadata = {
        #    "content": message,
        #     "last_updated": metadata["last_updated"]
        #}
        new_metadata = {"content": message, "last_updated": timestamp, "summary": ""}

        index.upsert([(new_id, get_embedding(message), new_metadata)])
        print(f"âœ… Táº¡o chá»§ Ä‘á» má»›i: {new_id}")


import time
last_message_time = time.time()
def should_close_topic(new_message):
    global last_message_time
    if new_message.strip() == "/done":
        return True
    elapsed_time = time.time() - last_message_time
    last_message_time = time.time()
    return elapsed_time > 500  # Sau 15 phÃºt khÃ´ng chat


def summarize_topic(topic_id):
    index = pc.Index(index_name)
    existing_metadata = fetch_metadata(topic_id)
    topic_text = existing_metadata["content"]

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "TÃ³m táº¯t ngáº¯n gá»n kiáº¿n thá»©c bÃ¡n hÃ ng tá»« há»™i thoáº¡i nÃ y."},
            {"role": "user", "content": topic_text}
        ]
    )
    summary = response.choices[0].message["content"]

    # Cáº­p nháº­t Pinecone vá»›i tÃ³m táº¯t
    updated_metadata = existing_metadata
    updated_metadata["summary"] = summary
    index.upsert([(topic_id, get_embedding(topic_text), updated_metadata)])

    print(f"ğŸ“Œ TÃ³m táº¯t lÆ°u vÃ o Pinecone: {summary}")

def process_message(message):
    if should_close_topic(message):
        print("ğŸ”¹ Chá»§ Ä‘á» káº¿t thÃºc, tá»•ng há»£p kiáº¿n thá»©c...")
        topic_id = find_similar_topic(message)
        if topic_id:
            summarize_topic(topic_id)
        return
    update_or_create_topic(message)
import numpy as np
from typing import List, Dict
def is_related(new_text: str, topic_embedding: List[float], threshold: float = 0.75) -> bool:
    new_embedding = np.array(get_embedding(new_text))
    topic_embedding = np.array(topic_embedding)
    similarity = np.dot(new_embedding, topic_embedding) / (np.linalg.norm(new_embedding) * np.linalg.norm(topic_embedding))
    return similarity >= threshold
def extract_new_skills(summary: str, existing_skills: list):
    """Tá»± Ä‘á»™ng phÃ¡t hiá»‡n skill má»›i tá»« ná»™i dung tÃ³m táº¯t."""
    extracted_skills = skill_extractor(summary)  # HÃ m nÃ y cáº§n Ä‘á»‹nh nghÄ©a
    new_skills = [skill for skill in extracted_skills if skill not in existing_skills]
    return new_skills
def skill_extractor(summary: str):
    """DÃ¹ng LLM Ä‘á»ƒ trÃ­ch xuáº¥t danh sÃ¡ch cÃ¡c skill tá»« ná»™i dung tÃ³m táº¯t."""
    prompt = f"""
    Dá»±a trÃªn Ä‘oáº¡n ná»™i dung sau, hÃ£y liá»‡t kÃª cÃ¡c ká»¹ nÄƒng (skills) cÃ³ thá»ƒ há»c Ä‘Æ°á»£c. 
    Chá»‰ tráº£ vá» danh sÃ¡ch cÃ¡c skill, khÃ´ng giáº£i thÃ­ch thÃªm.

    Ná»™i dung:
    "{summary}"

    Danh sÃ¡ch ká»¹ nÄƒng:
    """
    response = llm.invoke(prompt)
    return [skill.strip() for skill in response.content.split("\n") if skill.strip()]
