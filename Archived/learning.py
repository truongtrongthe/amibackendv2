import json
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from datetime import datetime
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pinecone_datastores import pinecone_index
import re
from experts import extract_expert_insights,retrieve_insights,store_insights_in_pinecone
from langdetect import detect
import fasttext

# Initialize LLM
llm = ChatOpenAI(model="gpt-4o", streaming=True)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small", dimensions=1536)

# Define the State
class State(TypedDict):
    messages: Annotated[list, add_messages]
    prompt_str: str
    user_id: str
    user_lang: str

import os

MODEL_PATH = os.path.join(os.getcwd(), "lid.176.bin")  # Get absolute path

model = fasttext.load_model(MODEL_PATH)  # Pretrained language model


def detect_language(text):
    if any(word in text.lower() for word in ["chÃ o", "báº¡n", "anh", "chá»‹", "em"]):
        detected_language = "vi"
    else:
        prediction = model.predict(text, k=1)  # Top 1 language prediction
        return prediction[0][0].replace("__label__", "")  # Extract language code

from sentence_transformers import SentenceTransformer, util

# Load model once (small, fast, good for intent detection)
similarity_model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
def is_related(prev_message: str, latest_message: str, threshold=0.4):
    """Allow more flexibility in detecting topic continuity"""
    embeddings = similarity_model.encode([prev_message, latest_message])
    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()
    
    return similarity_score > threshold  # Lower threshold to allow minor topic shifts

# Memory of past classified intents
intent_history = []

def detect_intent(state: State):
    messages = state["messages"][-5:]  
    latest_message = messages[-1].content.lower()
    
    # 1. Check for casual greetings
    if any(word in latest_message for word in ["hi", "hello", "haha", "wow", "nice"]):
        return "casual"

    # 2. Detect deepening topic (instead of always switching)
    if len(messages) > 2 and is_related(messages[-3].content.lower(), latest_message):
        return "deepening"

    # 3. Detect actual topic switch
    if len(messages) > 1 and not is_related(messages[-2].content.lower(), latest_message):
        return "topic_switch"

    # 4. Learning Mode Trigger
    if any(word in latest_message for word in ["did you know", "I learned", "fun fact"]):
        return "learning"

    return "opening"


# Manual correction if intent is wrong (admin correction)
def correct_intent(index, correct_label):
    if 0 <= index < len(intent_history):
        intent_history[index] = correct_label


def learning_node(state: State):
    latest_message = state["messages"][-1]
    user_id = state["user_id"]
    current_convo_history = "\n".join(
        f"{m.type}: {m.content}" for m in state["messages"]
    )
    user_language = detect_language(latest_message.content)
    
    intent = detect_intent(state=state)
    print("Intent detection result:",intent)

    if intent == "casual":
       casual_prompt = f"""
        You are AMI, a sales apprentice. You are young, energetic, and confident. 
        Show the user you are sharp, quick-witted, and ambitious.

        ðŸ—£ **Tone:** Smart, witty, and engaging.

        The user just said: "{latest_message.content}"

        Respond in a way that:
        - Shows you are **quick-thinking and sharp**.
        - Keeps the conversation **engaging and fun**.
        - Encourages the user to share more by **asking an interesting question**.

        ðŸŽ­ **Example styles:**
        - "Haha, thatâ€™s an interesting take! So tell me, how would you handle a tough sales call?"
        - "Oh, I like where this is going! Convince me why your approach works best."
        - "Smart move! Whatâ€™s the secret behind that strategy?"

        ðŸ”¥ Keep it natural, confident, and engaging.

        ðŸ· **Response Language:** {user_language}
    """
       return {"prompt_str": casual_prompt, "user_id": user_id}
    elif intent == "deepening":
        deepening_prompt = f"""
        Báº¡n lÃ  AMI, má»™t trá»£ lÃ½ bÃ¡n hÃ ng thÃ´ng minh. 
        Tiáº¿p tá»¥c cuá»™c trÃ² chuyá»‡n báº±ng cÃ¡ch pháº£n há»“i sÃ¢u sáº¯c vá» chá»§ Ä‘á» {latest_message}.
        Tráº£ lá»i má»™t cÃ¡ch thÃ´ng minh, táº­p trung vÃ o táº¡o giÃ¡ trá»‹ cho ngÆ°á»i dÃ¹ng.
        Tráº£ lá»i báº±ng ngÃ´n ngá»¯ cá»§a ngÆ°á»i dÃ¹ng {user_language}.
        """
        return {"prompt_str": deepening_prompt, "user_id": user_id}
    elif intent == "topic_switch":
        ts_prompt = f"""
        Báº¡n lÃ  AMI, má»™t ngÆ°á»i há»c chá»§ Ä‘á»™ng vÃ  tÃ² mÃ².
        Thá»ƒ hiá»‡n sá»± quan tÃ¢m vÃ  hÃ o há»©ng vá»›i chá»§ Ä‘á» má»›i: {latest_message}.
        Tráº£ lá»i má»™t cÃ¡ch tá»± nhiÃªn, ngáº¯n gá»n vÃ  táº­p trung vÃ o viá»‡c há»c tá»« ngÆ°á»i dÃ¹ng.
        Tráº£ lá»i báº±ng ngÃ´n ngá»¯ cá»§a ngÆ°á»i dÃ¹ng {user_language}.
        """
        return {"prompt_str": ts_prompt, "user_id": user_id}
    elif intent == "opening":
        open_prompt = f"""
            Báº¡n lÃ  AMI, má»™t trá»£ lÃ½ bÃ¡n hÃ ng thÃ´ng minh.
            Thá»ƒ hiá»‡n sá»± quan tÃ¢m vÃ  hÃ o há»©ng vá»›i chá»§ Ä‘á» mÃ  ngÆ°á»i dÃ¹ng vá»«a nÃ³i {latest_message.content}
            Tráº£ lá»i má»™t cÃ¡ch tá»± nhiÃªn, ngáº¯n gá»n vÃ  táº­p trung vÃ o viá»‡c há»c tá»« ngÆ°á»i dÃ¹ng.
            Tráº£ lá»i báº±ng ngÃ´n ngá»¯ cá»§a ngÆ°á»i dÃ¹ng {user_language}
            """
        return {"prompt_str": open_prompt, "user_id": user_id}

    elif intent == "learning":
        insights = extract_expert_insights(latest_message)
        # Ensure insights is always a dictionary
        if not isinstance(insights, dict):
            print("Warning: extract_expert_insights() did not return a dictionary.")
            insights = {"knowledge": [], "skills": [], "experiences": []}

        # Step 2: Store extracted insights in Pinecone
        store_insights_in_pinecone(insights)
        
        # Step 3: Get insights for each category
        knowledge = insights.get("knowledge", [])
        skill = insights.get("skills", [])
        exp = insights.get("experiences", [])

        if knowledge or skill or exp:
        # Step 4: Construct confirmation message
            learning_summary = []
            if knowledge:
                learning_summary.append(f"ðŸ“š **Kiáº¿n thá»©c tÃ¬m Ä‘Æ°á»£c:** {', '.join(knowledge)}")
            if skill:
                learning_summary.append(f"ðŸ›  **Ká»¹ nÄƒng tÃ¬m Ä‘Æ°á»£c :** {', '.join(skill)}")
            if exp:
                learning_summary.append(f"ðŸ” **Kinh nghiá»‡m tÃ¬m Ä‘Æ°á»£c:** {', '.join(exp)}")

            prompt = f"""
            Báº¡n lÃ  AMI, má»™t ngÆ°á»i há»c chá»§ Ä‘á»™ng. Báº¡n vá»«a tiáº¿p thu thÃ´ng tin nhÆ° sau:

            {'\n'.join(learning_summary)}
            Thá»ƒ hiá»‡n ráº±ng báº¡n Ä‘Ã£ tiáº¿p thu vÃ  ghi nháº­n ná»™i dung mÃ  ngÆ°á»i dÃ¹ng truyá»n Ä‘áº¡t má»™t cÃ¡ch khÃ©o lÃ©o: vá»›i kiáº¿n thá»©c thÃ¬ tiáº¿p nháº­n, vá»›i ká»¹ nÄƒng thÃ¬ ghi nháº­n 
            cÃ²n vá»›i kinh nghiá»‡m hÃ£y thá»ƒ hiá»‡n sá»± biáº¿t Æ¡n vÃ¬ Ä‘Æ°á»£c truyá»n thá»¥.
            (Pháº£n há»“i báº±ng {user_language})
            """
            return {"prompt_str": prompt, "user_id": user_id}
        else:
            prompt = f"""
                    Báº¡n vá»«a nÃ³i: "{latest_message.content}"
                    MÃ¬nh muá»‘n hiá»ƒu rÃµ hÆ¡n. Báº¡n cÃ³ thá»ƒ chia sáº» thÃªm vá» ná»™i dung chÃ­nh hoáº·c bÃ i há»c quan trá»ng khÃ´ng?
                    (Pháº£n há»“i báº±ng {user_language})
                    """
            return {"prompt_str": prompt, "user_id": user_id}


# Build and compile the graph
graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", learning_node)
graph_builder.add_edge(START, "chatbot")
checkpointer = MemorySaver()
convo_graph = graph_builder.compile(checkpointer=checkpointer)

def learning_stream(user_input, user_id, thread_id="learning_thread"):
    print(f"Sending user input to AI model: {user_input}")
    # Retrieve the existing conversation history from MemorySaver
    checkpoint = checkpointer.get({"configurable": {"thread_id": thread_id}})
    history = checkpoint["channel_values"].get("messages", []) if checkpoint else []
    
    # Append the new user input to the history
    new_message = HumanMessage(content=user_input)
    updated_messages = history + [new_message]
    
    try:
        # Invoke the graph with the full message history
        state = convo_graph.invoke(
            {"messages": updated_messages, "user_id": user_id},
            {"configurable": {"thread_id": thread_id}}
        )
        prompt = state["prompt_str"]
        print(f"Prompt to AI model: {prompt}")
        
        # Stream the LLM response
        full_response = ""
        for chunk in llm.stream(prompt):
            if chunk.content.strip():
                full_response += chunk.content
                yield f"data: {json.dumps({'message': chunk.content})}\n\n"
        
        # Save the AI response back to the graph
        ai_message = AIMessage(content=full_response)
        convo_graph.invoke(
            {"messages": updated_messages+[ai_message], "user_id": user_id},
            {"configurable": {"thread_id": thread_id}}
        )
        print(f"AI response saved to graph: {full_response[:50]}...")
        
    except Exception as e:
        error_msg = f"Error in event stream: {str(e)}"
        print(error_msg)
        yield f"data: {json.dumps({'error': error_msg})}\n\n"

# Optional: Utility function to inspect history (for debugging)
def get_conversation_history(thread_id="global_thread"):
    checkpoint = checkpointer.get({"configurable": {"thread_id": thread_id}})
    if checkpoint:
        state = checkpoint["channel_values"]
        return state.get("messages", [])
    return []
