from langchain_openai import ChatOpenAI
from Archived.knowledge import  retrieve_relevant_infov2 
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
import os
from langchain_openai import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
import json
import logging
from typing import Dict, Any
from openai import OpenAI
logging.basicConfig(level=logging.INFO)

# Declare user_context as a global variable
user_context = {"chat_history": [], "customer_info": {}}

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "")
PINECONE_ENV = "us-east-1"  # Check Pinecone console for your region
index_name = "ami-knowledge"
llm = ChatOpenAI(model="gpt-4o", streaming=True)
client = OpenAI()
prompt = PromptTemplate(
    input_variables=["history", "user_input", "products", "user_style", "sales_skills"],
    template="""
    ğŸ¯ **Má»¥c tiÃªu**: Hiá»ƒu Ã½ Ä‘á»‹nh cá»§a ngÆ°á»i dÃ¹ng vÃ  pháº£n há»“i má»™t cÃ¡ch phÃ¹ há»£p.

    1ï¸âƒ£ **Náº¿u ngÆ°á»i dÃ¹ng Ä‘ang há»i vá» sáº£n pháº©m** â†’ Dá»±a vÃ o thÃ´ng tin sáº£n pháº©m Ä‘Ã£ tÃ¬m tháº¥y ({products}) Ä‘á»ƒ tÆ° váº¥n ngáº¯n gá»n, Ä‘á»§ Ã½, cÃ³ dáº«n dáº¯t há»£p lÃ½.  
    2ï¸âƒ£ **Náº¿u ngÆ°á»i dÃ¹ng Ä‘ang há»i vá» ká»¹ nÄƒng bÃ¡n hÃ ng** â†’ Ãp dá»¥ng ká»¹ nÄƒng phÃ¹ há»£p tá»« ({sales_skills}) vÃ o cÃ¢u tráº£ lá»i.  
    3ï¸âƒ£ **Náº¿u ngÆ°á»i dÃ¹ng Ä‘ang trÃ² chuyá»‡n bÃ¬nh thÆ°á»ng** â†’ Duy trÃ¬ há»™i thoáº¡i má»™t cÃ¡ch tá»± nhiÃªn, cÃ³ thá»ƒ thÃªm cÃ¢u há»i gá»£i má»Ÿ.  
    4ï¸âƒ£ **LuÃ´n pháº£n há»“i theo phong cÃ¡ch cá»§a ngÆ°á»i dÃ¹ng trÆ°á»›c Ä‘Ã¢y**: {user_style}  

    ğŸ“œ **Lá»‹ch sá»­ cuá»™c trÃ² chuyá»‡n**:  
    {history}  

    ğŸ—£ **Tin nháº¯n tá»« ngÆ°á»i dÃ¹ng**:  
    "{user_input}"  

    âœï¸ **Pháº£n há»“i cá»§a AMI** (giá»¯ phong cÃ¡ch há»™i thoáº¡i phÃ¹ há»£p):  
    """
)

#  chat_history = ChatMessageHistory()
chat_history = ChatMessageHistory()

memory = ConversationBufferMemory(
    chat_memory=chat_history,
    memory_key="history",  # REQUIRED in newer versions
    return_messages=True
)

def retrieve_product(user_input):
    """Retrieve relevant context from Pinecone and return a structured summary."""
    if user_input is None:
        return "KhÃ´ng tÃ¬m tháº¥y sáº£n pháº©m phÃ¹ há»£p."  # Return an appropriate message if input is None

    retrieved_info = retrieve_relevant_infov2(user_input, top_k=3)

    if not retrieved_info:
        return "KhÃ´ng tÃ¬m tháº¥y sáº£n pháº©m phÃ¹ há»£p."

    structured_summary = []
    for doc in retrieved_info:
        content = doc.get("content", "").strip()
        if content:
            structured_summary.append(content)
    return "\n\n".join(structured_summary) if structured_summary else "KhÃ´ng tÃ¬m tháº¥y sáº£n pháº©m phÃ¹ há»£p."

pc = Pinecone(api_key=PINECONE_API_KEY)

# Check if index exists
existing_indexes = [i['name'] for i in pc.list_indexes()]
if index_name not in existing_indexes:
    pc.create_index(
        name=index_name,
        dimension=1536,  # Ensure this matches your model's output dimension
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

def detect_customer_intent_dynamic(message: str) -> Dict[str, Any]:
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": """You are an AI that detects customer intent and categorizes it into three main groups:
                1. general_conversation (e.g., greetings, small talk, general inquiries)
                2. sales_related (e.g., asking about price, product details, promotions)
                3. after_sales (e.g., warranty, support, returns, complaints)
                Return a JSON object with "intent", "intent_group", and optional "sub_intent" fields.
            """},
            {"role": "user", "content": f"Analyze intent from this message: {message}"}
        ]
    )
    intent_data = response.choices[0].message.content.strip()
    
    # Attempt to parse the intent_data as JSON
    try:
        return json.loads(intent_data)  # Use json.loads instead of eval
    except json.JSONDecodeError as e:
        print("JSON decoding error:", e)
        return {"intent": "unknown", "intent_group": "general_conversation"}  # Return a default value in case of error
   
def ami_drive(user_message, user_context,company_goal,product_info):
    """
    Xá»­ lÃ½ tin nháº¯n tá»« ngÆ°á»i dÃ¹ng, xÃ¡c Ä‘á»‹nh má»¥c tiÃªu, táº¡o Best_map vÃ  dáº«n dáº¯t há»™i thoáº¡i.
    """
    intent_data = detect_customer_intent_dynamic(user_message)
    intent = intent_data.get("intent", "unknown")
    intent_group = intent_data.get("intent_group", "general_conversation")  # Máº·c Ä‘á»‹nh lÃ  giao tiáº¿p thÃ´ng thÆ°á»ng
    sub_intent = intent_data.get("sub_intent", None)
    print("intent_group in the ami_drive:", intent_group)
    if intent_group == "general_conversation":
        return handle_general_conversation(intent, sub_intent,user_message, user_context)

    elif intent_group == "sales_related":
        return handle_sales(user_message, user_context,company_goal,product_info)

    elif intent_group == "after_sales":
        return "Post-sales support"

    else:
        return "Xin lá»—i, tÃ´i chÆ°a hiá»ƒu rÃµ cÃ¢u há»i cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n khÃ´ng?"

def handle_general_conversation(intent, sub_intent, user_message, user_context):
    # Láº¥y thÃ´ng tin khÃ¡ch hÃ ng tá»« user_context
    customer_info = user_context.get("customer_info", {})
    chat_history = user_context.get("chat_history", [])

    # Append the new user message to the chat history
    chat_history.append(f"User: {user_message}")
    user_context["chat_history"] = chat_history  

    # **Danh sÃ¡ch thÃ´ng tin cáº§n thu tháº­p**
    required_fields = ["name", "age", "occupation", "interests"]
    missing_fields = [field for field in required_fields if not customer_info.get(field)]

    # **Chá»‰ há»i tÃªn náº¿u tháº­t sá»± chÆ°a cÃ³**
    if "name" in missing_fields:
        probing_prompt = f"""
        Lá»‹ch sá»­ há»™i thoáº¡i: {', '.join(chat_history)}  # Join the list into a string for display
        ThÃ´ng tin khÃ¡ch hÃ ng hiá»‡n cÃ³: {customer_info}
        Báº¡n lÃ  má»™t trá»£ lÃ½ AI. KhÃ¡ch hÃ ng chÆ°a cung cáº¥p tÃªn. HÃ£y Ä‘áº·t má»™t cÃ¢u há»i lá»‹ch sá»± Ä‘á»ƒ há»i tÃªn.
        """
        return llm.invoke(probing_prompt).content

    # **Náº¿u Ä‘Ã£ cÃ³ tÃªn nhÆ°ng cÃ²n thiáº¿u thÃ´ng tin khÃ¡c â†’ Há»i tiáº¿p thÃ´ng tin cÃ²n thiáº¿u**
    if missing_fields:
        probing_prompt = f"""
        Lá»‹ch sá»­ há»™i thoáº¡i: {', '.join(chat_history)}  # Join the list into a string for display
        ThÃ´ng tin khÃ¡ch hÃ ng hiá»‡n cÃ³: {customer_info}
        ThÃ´ng tin cÃ²n thiáº¿u: {missing_fields}
        HÃ£y Ä‘áº·t má»™t cÃ¢u há»i tá»± nhiÃªn Ä‘á»ƒ khai thÃ¡c má»™t trong cÃ¡c thÃ´ng tin cÃ²n thiáº¿u mÃ  khÃ´ng lÃ m khÃ¡ch hÃ ng khÃ³ chá»‹u.
        """
        return llm.invoke(probing_prompt).content

    # **Náº¿u Ä‘Ã£ cÃ³ Ä‘á»§ thÃ´ng tin â†’ Tráº£ lá»i theo ngá»¯ cáº£nh**
    response_prompt = f"""
    TÃ³m táº¯t há»™i thoáº¡i: {', '.join(chat_history)}  # Join the list into a string for display
    ThÃ´ng tin khÃ¡ch hÃ ng: {customer_info}
    CÃ¢u khÃ¡ch hÃ ng vá»«a há»i: {user_message}
    HÃ£y pháº£n há»“i má»™t cÃ¡ch tá»± nhiÃªn, phÃ¹ há»£p vá»›i thÃ´ng tin khÃ¡ch hÃ ng, giá»¯ cuá»™c trÃ² chuyá»‡n mÆ°á»£t mÃ .
    """
    extract_prompt = f"""
    Há»™i thoáº¡i: {', '.join(chat_history)}  # Join the list into a string for display
    ThÃ´ng tin hiá»‡n cÃ³: {user_context.get("customer_info", {})}
    HÃ£y cáº­p nháº­t thÃ´ng tin khÃ¡ch hÃ ng dá»±a trÃªn há»™i thoáº¡i má»›i. 
    ChÃº Ã½: Náº¿u Ä‘Ã£ cÃ³ thÃ´ng tin, khÃ´ng Ä‘Æ°á»£c lÃ m máº¥t thÃ´ng tin cÅ©. Chá»‰ bá»• sung pháº§n cÃ²n thiáº¿u.
    """

    return llm.invoke(response_prompt).content
    
    
def handle_sales(user_message, user_context,company_goal,product_info):
    """
    Xá»­ lÃ½ tin nháº¯n tá»« ngÆ°á»i dÃ¹ng, xÃ¡c Ä‘á»‹nh má»¥c tiÃªu, táº¡o Best_map vÃ  dáº«n dáº¯t há»™i thoáº¡i.
    """
    # BÆ°á»›c 1: Láº¥y thÃ´ng tin khÃ¡ch hÃ ng tá»« user_context
    customer_info = user_context.get("customer_info", {})
    chat_history = user_context.get("chat_history", [])
    chat_history.append(f"User: {user_message}")
    user_context["chat_history"] = chat_history  # Cáº­p nháº­t lá»‹ch sá»­ há»™i thoáº¡i
    
    # BÆ°á»›c 2: XÃ¡c Ä‘á»‹nh customer_stage tá»« lá»‹ch sá»­ há»™i thoáº¡i
    customer_stage = get_customer_stage(chat_history)
    user_context["customer_stage"] = customer_stage
    print("customer_stage:", customer_stage)

    # BÆ°á»›c 3: XÃ¡c Ä‘á»‹nh má»¥c tiÃªu há»™i thoáº¡i
    cg = get_conversation_goal(customer_info, user_message, customer_stage)
    print("conversation_goal in handle_user_message:", cg)

    # BÆ°á»›c 3: Cáº­p nháº­t customer_info vá»›i customer_stage
    customer_info["customer_stage"] = customer_stage
    # BÆ°á»›c 4: Táº¡o Best_map
    best_map = create_best_map(cg, customer_info,company_goal,product_info)
    print("best_map in handle_user_message:", best_map)

    response = generate_response(best_map, company_goal, customer_info)
    return response

def get_customer_stage(chat_history, company_goal="khÃ¡ch chuyá»ƒn khoáº£n"):
    """
    DÃ¹ng LLM Ä‘á»ƒ xÃ¡c Ä‘á»‹nh giai Ä‘oáº¡n cá»§a khÃ¡ch hÃ ng dá»±a trÃªn lá»‹ch sá»­ há»™i thoáº¡i.
    """
    prompt = f"""
    Báº¡n lÃ  má»™t AI tÆ° váº¥n bÃ¡n hÃ ng. DÆ°á»›i Ä‘Ã¢y lÃ  lá»‹ch sá»­ há»™i thoáº¡i giá»¯a nhÃ¢n viÃªn vÃ  khÃ¡ch hÃ ng:
    {chat_history}
    
    CÃ´ng ty cÃ³ má»¥c tiÃªu cuá»‘i cÃ¹ng lÃ  '{company_goal}'.
    Dá»±a vÃ o lá»‹ch sá»­ há»™i thoáº¡i, hÃ£y xÃ¡c Ä‘á»‹nh khÃ¡ch hÃ ng Ä‘ang á»Ÿ giai Ä‘oáº¡n nÃ o trong hÃ nh trÃ¬nh nÃ y:
    - Awareness (Nháº­n thá»©c)
    - Interest (Quan tÃ¢m)
    - Consideration (CÃ¢n nháº¯c)
    - Decision (Quyáº¿t Ä‘á»‹nh)
    - Action (Chuyá»ƒn khoáº£n)
    
    Chá»‰ tráº£ vá» má»™t trong cÃ¡c giai Ä‘oáº¡n trÃªn mÃ  khÃ´ng cÃ³ báº¥t ká»³ giáº£i thÃ­ch nÃ o.
    """

    response= llm.invoke(prompt).content
    return response.strip()


def customer_emotion(chat_history):
   
    prompt = f"""
    Báº¡n lÃ  má»™t chuyÃªn gia tÃ¢m lÃ½ tinh táº¿. HÃ£y phÃ¡t hiá»‡n cáº£m xÃºc hiá»‡n táº¡i cá»§a khÃ¡ch hÃ ng dá»±a trÃªn lá»‹ch sá»­ há»™i thoáº¡i:
    {chat_history}
    Chá»‰ tráº£ vá» tráº¡ng thÃ¡i cáº£m xÃºc mÃ  khÃ´ng giáº£i thÃ­ch thÃªm
    """
    response= llm.invoke(prompt).content
    return response.strip()

def get_customer_info(chat_history):
    """
    DÃ¹ng LLM Ä‘á»ƒ phÃ¢n tÃ­ch lá»‹ch sá»­ há»™i thoáº¡i vÃ  trÃ­ch xuáº¥t thÃ´ng tin khÃ¡ch hÃ ng.
    """
    # Join the chat history into a single string for processing
    chat_history_str = "\n".join(user_context["chat_history"])
    print("Formatted chat_history_str:", repr(chat_history_str))

    
 

    prompt = f"""
DÆ°á»›i Ä‘Ã¢y lÃ  lá»‹ch sá»­ há»™i thoáº¡i giá»¯a AI vÃ  khÃ¡ch hÃ ng:
{chat_history_str}

HÃ£y cá»‘ gáº¯ng suy luáº­n thÃ´ng tin tá»« cuá»™c trÃ² chuyá»‡n nÃ y.
Náº¿u khÃ¡ch hÃ ng Ä‘á» cáº­p Ä‘áº¿n má»™t tÃªn riÃªng, giáº£ Ä‘á»‹nh Ä‘Ã³ lÃ  "name".
Náº¿u khÃ¡ch hÃ ng nÃ³i vá» cÃ´ng viá»‡c cá»§a há», Ä‘Ã³ lÃ  "occupation".
Náº¿u chÆ°a cÃ³ Ä‘á»§ thÃ´ng tin, hÃ£y dá»± Ä‘oÃ¡n dá»±a trÃªn ngá»¯ cáº£nh hoáº·c Ä‘á»ƒ trá»‘ng.

Tráº£ vá» má»™t JSON vá»›i cÃ¡c trÆ°á»ng:
- name (náº¿u cÃ³ thá»ƒ suy luáº­n)
- age (náº¿u cÃ³ thá»ƒ suy luáº­n)
- gender (náº¿u cÃ³ thá»ƒ suy luáº­n)
- occupation (náº¿u cÃ³ thá»ƒ suy luáº­n)
- interests (náº¿u cÃ³ thá»ƒ suy luáº­n)
- purchase_history (náº¿u cÃ³ thá»ƒ suy luáº­n)
"""


    response = llm.invoke(prompt).content  # Gá»i LLM Ä‘á»ƒ phÃ¢n tÃ­ch
    print("response in the get_customer_info:", response)

    try:
        # Clean the JSON string if it has extra formatting
        json_start = response.find("{")
        json_end = response.rfind("}") + 1
        clean_json = response[json_start:json_end]

        # Parse JSON
        extracted_info = json.loads(clean_json)
        print("Extracted customer info:", extracted_info)

        return extracted_info
    except json.JSONDecodeError as e:
        print("JSON decoding error:", e)
        return {}

def update_customer_info(current_info, new_info):
    """
    Cáº­p nháº­t thÃ´ng tin khÃ¡ch hÃ ng vá»›i dá»¯ liá»‡u má»›i mÃ  khÃ´ng lÃ m máº¥t thÃ´ng tin cÅ©.
    """
    for key, value in new_info.items():
        if value:  # Chá»‰ cáº­p nháº­t náº¿u cÃ³ thÃ´ng tin má»›i
            current_info[key] = value
    
    # Kiá»ƒm tra náº¿u váº«n cÃ²n missing fields
    missing_fields = [key for key, value in current_info.items() if not value]
    if missing_fields:
        current_info["status"] = "missing_info"
        current_info["missing_fields"] = missing_fields
    else:
        current_info["status"] = "completed"

    return current_info

def chat_pipeline(user_message, chat_history, customer_info, llm):
   
    # 1. TrÃ­ch xuáº¥t thÃ´ng tin tá»« lá»‹ch sá»­ chat
    new_info = customer_info(chat_history, llm)
    print("new_info:", new_info)

    # 2. Cáº­p nháº­t thÃ´ng tin khÃ¡ch hÃ ng
    customer_info = update_customer_info(customer_info, new_info)
    print("customer_info:", customer_info)
    # 3. Kiá»ƒm tra xem Ä‘Ã£ Ä‘á»§ thÃ´ng tin chÆ°a
    if customer_info["status"] == "missing_info":
        missing = customer_info["missing_fields"]
        next_question = ask_for_missing_info(missing)
        return next_question, customer_info
    
    # 4. Náº¿u Ä‘Ã£ Ä‘á»§ thÃ´ng tin, tiáº¿p tá»¥c há»™i thoáº¡i
    response = continue_conversation(user_message, customer_info, llm)
    return response, customer_info
def ask_for_missing_info(missing_fields):
    """
    Sinh cÃ¢u há»i Ä‘á»ƒ tiáº¿p tá»¥c hoÃ n thiá»‡n thÃ´ng tin khÃ¡ch hÃ ng.
    """
    questions = {
        "name": "Báº¡n cÃ³ thá»ƒ cho tÃ´i biáº¿t tÃªn cá»§a báº¡n khÃ´ng?",
        "age": "Báº¡n bao nhiÃªu tuá»•i?",
        "gender": "Báº¡n lÃ  nam hay ná»¯?",
        "occupation": "Báº¡n Ä‘ang lÃ m nghá» gÃ¬?",
        "interests": "Báº¡n quan tÃ¢m Ä‘áº¿n lÄ©nh vá»±c nÃ o?",
        "purchase_history": "Báº¡n Ä‘Ã£ tá»«ng mua sáº£n pháº©m nÃ o tÆ°Æ¡ng tá»± chÆ°a?"
    }
    for field in missing_fields:
        if field in questions:
            return questions[field]  # Há»i láº§n lÆ°á»£t tá»«ng cÃ¢u
    return "HÃ£y cho tÃ´i biáº¿t thÃªm vá» báº¡n!"  # Náº¿u khÃ´ng cÃ³ cÃ¢u há»i cá»¥ thá»ƒ
def continue_conversation(user_message, customer_info, llm):
    """
    Tiáº¿p tá»¥c há»™i thoáº¡i dá»±a trÃªn thÃ´ng tin khÃ¡ch hÃ ng vÃ  má»¥c tiÃªu há»™i thoáº¡i.
    """
    cg = get_conversation_goal(user_message, customer_info)
    
    best_map = create_best_map(cg, customer_info)
    
    response = llm.generate(f"Dá»±a vÃ o má»¥c tiÃªu '{cg}', hÃ£y pháº£n há»“i: {user_message}")
    
    return response

def get_conversation_goal(customer_info, user_message, customer_stage):
    """
    XÃ¡c Ä‘á»‹nh má»¥c tiÃªu há»™i thoáº¡i dá»±a trÃªn thÃ´ng tin khÃ¡ch hÃ ng, ná»™i dung tin nháº¯n vÃ  giai Ä‘oáº¡n khÃ¡ch hÃ ng.
    """
    print("user_message in the determine_conversation_goal:", user_message)
    print("customer_info in the determine_conversation_goal:", customer_info)
    print("customer_stage in the determine_conversation_goal:", customer_stage)

    # Náº¿u thÃ´ng tin khÃ¡ch cÃ²n thiáº¿u, cáº§n tiáº¿p tá»¥c há»i Ä‘á»ƒ hoÃ n chá»‰nh
    if "missing_fields" in customer_info and len(customer_info["missing_fields"]) > 0:
        return "Khá»Ÿi táº¡o há»™i thoáº¡i chung"

    # XÃ¡c Ä‘á»‹nh má»¥c tiÃªu tiáº¿p theo báº±ng cÃ¡ch suy luáº­n tá»« company_goal
    prompt = f"""
    Dá»±a trÃªn giai Ä‘oáº¡n khÃ¡ch hÃ ng trong hÃ nh trÃ¬nh mua hÃ ng: "{customer_stage}", 
    vÃ  tin nháº¯n: "{user_message}", hÃ£y xÃ¡c Ä‘á»‹nh bÆ°á»›c há»£p lÃ½ tiáº¿p theo Ä‘á»ƒ dáº«n khÃ¡ch hÃ ng Ä‘áº¿n má»¥c tiÃªu "Chuyá»ƒn khoáº£n".
    
    Tráº£ vá» chá»‰ má»™t má»¥c tiÃªu há»™i thoáº¡i cá»¥ thá»ƒ (khÃ´ng giáº£i thÃ­ch), vÃ­ dá»¥: "Giá»›i thiá»‡u sáº£n pháº©m", "Thuyáº¿t phá»¥c khÃ¡ch hÃ ng", "HÆ°á»›ng dáº«n thanh toÃ¡n".
    """

    response = llm.invoke(prompt).content  # Gá»i LLM Ä‘á»ƒ phÃ¢n tÃ­ch


    return response.strip()

def create_best_map(conversation_goal, customer_info, company_goal, product_info):
    """
    Sá»­ dá»¥ng LLM Ä‘á»ƒ suy luáº­n Best_map phÃ¹ há»£p dá»±a trÃªn conversation_goal, customer_info vÃ  company_goal.
    """
    prompt = f"""
    ğŸ›’ KhÃ¡ch hÃ ng Ä‘ang á»Ÿ giai Ä‘oáº¡n: "{customer_info.get('customer_stage', 'Unknown')}"
    ğŸ¯ Má»¥c tiÃªu há»™i thoáº¡i: "{conversation_goal}"
    ğŸ† Má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a cÃ´ng ty: "{company_goal}"
    ğŸ‘¤ ThÃ´ng tin khÃ¡ch hÃ ng: {customer_info}
    ğŸ“¦ ThÃ´ng tin sáº£n pháº©m cÃ´ng ty: {product_info}

    âœ… HÃ£y táº¡o má»™t hÆ°á»›ng dáº«n pháº£n há»“i tá»‘t nháº¥t (Best_map) giÃºp nhÃ¢n viÃªn bÃ¡n hÃ ng nÃ³i chuyá»‡n há»£p lÃ½ vÃ  hÆ°á»›ng khÃ¡ch hÃ ng Ä‘áº¿n {company_goal}.
    âœ… Äiá»u chá»‰nh pháº£n há»“i dá»±a trÃªn cáº£m xÃºc vÃ  giai Ä‘oáº¡n cá»§a khÃ¡ch hÃ ng:
    - Náº¿u chÆ°a biáº¿t tÃªn khÃ¡ch hÃ ng, hÃ£y há»i tÃªn khÃ¡ch hÃ ng trÆ°á»›c.
    - Náº¿u khÃ¡ch hÃ ng cÃ²n phÃ¢n vÃ¢n, hÃ£y nháº¥n máº¡nh lá»£i Ã­ch cá»§a sáº£n pháº©m.
    - Náº¿u khÃ¡ch hÃ ng cÃ³ há»©ng thÃº, hÃ£y gá»£i má»Ÿ má»™t lÃ½ do máº¡nh máº½ Ä‘á»ƒ hÃ nh Ä‘á»™ng ngay.
    - Náº¿u khÃ¡ch hÃ ng cÃ³ lo ngáº¡i, hÃ£y tráº¥n an vÃ  cung cáº¥p thÃ´ng tin há»— trá»£.

    ğŸ¤ Náº¿u biáº¿t tÃªn khÃ¡ch hÃ ng, hÃ£y xÆ°ng hÃ´ thÃ¢n thiá»‡n.
    ğŸ“¢ Tráº£ vá» má»™t Ä‘oáº¡n vÄƒn ngáº¯n, khÃ´ng quÃ¡ 3 cÃ¢u, vá»›i phong cÃ¡ch giao tiáº¿p thÆ°á»ng thá»©c (casual).
    """
    response = llm.invoke(prompt).content  # Gá»i OpenAI hoáº·c mÃ´ hÃ¬nh AI khÃ¡c
    return response.strip()


def search_sales_skills(query_text, max_skills=3):
    """ 
    Truy váº¥n ká»¹ nÄƒng tá»« Pinecone vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n. 
    """
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-large", dimensions=1536)
    query_embedding = embedding_model.embed_query(query_text)

    index = pc.Index(index_name)
    response = index.query(
        vector=query_embedding,
        top_k=max_skills,
        include_metadata=True  
    )

    skills = []
    if response and "matches" in response:
        for match in response["matches"]:
            skill_text = match.get("metadata", {}).get("content")
            if skill_text:
                skills.append(skill_text)

    return skills if skills else ["KhÃ´ng tÃ¬m tháº¥y ká»¹ nÄƒng phÃ¹ há»£p."]



chain = (
    RunnablePassthrough.assign(
        history=lambda _: memory.load_memory_variables({}).get("history", []),
        products=lambda x: retrieve_product(x["user_input"]),
        sales_skills=lambda x: ", ".join(search_sales_skills(x["user_input"], max_skills=3)),  # Láº¥y ká»¹ nÄƒng tá»« Pinecone
        user_style=lambda _: "lá»‹ch sá»±"
    )  
    | prompt
    | llm
)



def ami_selling(user_message):
    """
    HÃ m chÃ­nh xá»­ lÃ½ há»™i thoáº¡i bÃ¡n hÃ ng cá»§a Ami.
    """
    global user_context  # Declare user_context as global

    print("user_message in the ami_selling:", user_message)

    # Append the new user message to the chat history
    print("Before appending message:", user_context["chat_history"])
    user_context["chat_history"].append(f"User: {user_message}")
    print("After appending message:", user_context["chat_history"])


    # TrÃ­ch xuáº¥t thÃ´ng tin khÃ¡ch hÃ ng
    extracted_info = get_customer_info(user_message)
    print("extracted_info in the ami_selling:", extracted_info)

    # Cáº­p nháº­t user_context vá»›i customer_info má»›i
    user_context["customer_info"] = update_customer_info(user_context.get("customer_info", {}), extracted_info)

    print("Updated user_context:", user_context)

    company_goal = "KhÃ¡ch chuyá»ƒn khoáº£n"
    product_info = retrieve_product(user_message)

    # Gá»i handle_user_message Ä‘á»ƒ láº¥y pháº£n há»“i chÃ­nh theo Best_map
    response = ami_drive(user_message, user_context, company_goal, product_info)

    return response

def generate_response(best_map, company_goal, customer_info):
    """
    Sinh pháº£n há»“i dá»±a trÃªn Best_map + hÆ°á»›ng khÃ¡ch hÃ ng Ä‘áº¿n company_goal.
    """
    prompt = f"""
    KhÃ¡ch hÃ ng: {customer_info}
    Best_map: "{best_map}"
    Company_goal: "{company_goal}"

    HÃ£y táº¡o má»™t pháº£n há»“i tá»± nhiÃªn, thÃ¢n thiá»‡n, dáº«n dáº¯t khÃ¡ch hÃ ng theo Best_map vÃ  hÆ°á»›ng há» Ä‘áº¿n {company_goal}.
    """

    response = llm.invoke(prompt).content  # Gá»i OpenAI hoáº·c mÃ´ hÃ¬nh AI khÃ¡c Ä‘á»ƒ sinh pháº£n há»“i
    return response.strip()