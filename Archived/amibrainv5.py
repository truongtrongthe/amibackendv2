from langchain_openai import ChatOpenAI
from Archived.knowledge import  retrieve_relevant_infov2 
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
import os
from langchain_openai import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
import json
import logging
from typing import Dict, Any
from openai import OpenAI
logging.basicConfig(level=logging.INFO)

# Declare user_context as a global variable
user_context = {"chat_history": [], "customer_info": {}}

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "")
PINECONE_ENV = "us-east-1"  # Check Pinecone console for your region
index_name = "ami-knowledge"
llm = ChatOpenAI(model="gpt-4o", streaming=True)
client = OpenAI()
prompt = PromptTemplate(
    input_variables=["history", "user_input", "products", "user_style", "sales_skills"],
    template="""
    üéØ **M·ª•c ti√™u**: Hi·ªÉu √Ω ƒë·ªãnh c·ªßa ng∆∞·ªùi d√πng v√† ph·∫£n h·ªìi m·ªôt c√°ch ph√π h·ª£p.

    1Ô∏è‚É£ **N·∫øu ng∆∞·ªùi d√πng ƒëang h·ªèi v·ªÅ s·∫£n ph·∫©m** ‚Üí D·ª±a v√†o th√¥ng tin s·∫£n ph·∫©m ƒë√£ t√¨m th·∫•y ({products}) ƒë·ªÉ t∆∞ v·∫•n ng·∫Øn g·ªçn, ƒë·ªß √Ω, c√≥ d·∫´n d·∫Øt h·ª£p l√Ω.  
    2Ô∏è‚É£ **N·∫øu ng∆∞·ªùi d√πng ƒëang h·ªèi v·ªÅ k·ªπ nƒÉng b√°n h√†ng** ‚Üí √Åp d·ª•ng k·ªπ nƒÉng ph√π h·ª£p t·ª´ ({sales_skills}) v√†o c√¢u tr·∫£ l·ªùi.  
    3Ô∏è‚É£ **N·∫øu ng∆∞·ªùi d√πng ƒëang tr√≤ chuy·ªán b√¨nh th∆∞·ªùng** ‚Üí Duy tr√¨ h·ªôi tho·∫°i m·ªôt c√°ch t·ª± nhi√™n, c√≥ th·ªÉ th√™m c√¢u h·ªèi g·ª£i m·ªü.  
    4Ô∏è‚É£ **Lu√¥n ph·∫£n h·ªìi theo phong c√°ch c·ªßa ng∆∞·ªùi d√πng tr∆∞·ªõc ƒë√¢y**: {user_style}  

    üìú **L·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán**:  
    {history}  

    üó£ **Tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng**:  
    "{user_input}"  

    ‚úçÔ∏è **Ph·∫£n h·ªìi c·ªßa AMI** (gi·ªØ phong c√°ch h·ªôi tho·∫°i ph√π h·ª£p):  
    """
)

#  chat_history = ChatMessageHistory()
chat_history = ChatMessageHistory()

memory = ConversationBufferMemory(
    chat_memory=chat_history,
    memory_key="history",  # REQUIRED in newer versions
    return_messages=True
)

def retrieve_product(user_input):
    """Retrieve relevant context from Pinecone and return a structured summary."""
    if user_input is None:
        return "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p."  # Return an appropriate message if input is None

    retrieved_info = retrieve_relevant_infov2(user_input, top_k=3)

    if not retrieved_info:
        return "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p."

    structured_summary = []
    for doc in retrieved_info:
        content = doc.get("content", "").strip()
        if content:
            structured_summary.append(content)
    return "\n\n".join(structured_summary) if structured_summary else "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p."

pc = Pinecone(api_key=PINECONE_API_KEY)

# Check if index exists
existing_indexes = [i['name'] for i in pc.list_indexes()]
if index_name not in existing_indexes:
    pc.create_index(
        name=index_name,
        dimension=1536,  # Ensure this matches your model's output dimension
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

def detect_customer_intent_dynamic(message: str) -> Dict[str, Any]:
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": """You are an AI that detects customer intent and categorizes it into three main groups:
                1. general_conversation (e.g., greetings, small talk, general inquiries)
                2. sales_related (e.g., asking about price, product details, promotions)
                3. after_sales (e.g., warranty, support, returns, complaints)
                Return a JSON object with "intent", "intent_group", and optional "sub_intent" fields.
            """},
            {"role": "user", "content": f"Analyze intent from this message: {message}"}
        ]
    )
    intent_data = response.choices[0].message.content.strip()
    
    # Attempt to parse the intent_data as JSON
    try:
        return json.loads(intent_data)  # Use json.loads instead of eval
    except json.JSONDecodeError as e:
        print("JSON decoding error:", e)
        return {"intent": "unknown", "intent_group": "general_conversation"}  # Return a default value in case of error
   
def ami_drive(user_message, user_context,company_goal,product_info):
    """
    X·ª≠ l√Ω tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng, x√°c ƒë·ªãnh m·ª•c ti√™u, t·∫°o Best_map v√† d·∫´n d·∫Øt h·ªôi tho·∫°i.
    """
    intent_data = detect_customer_intent_dynamic(user_message)
    intent = intent_data.get("intent", "unknown")
    intent_group = intent_data.get("intent_group", "general_conversation")  # M·∫∑c ƒë·ªãnh l√† giao ti·∫øp th√¥ng th∆∞·ªùng
    sub_intent = intent_data.get("sub_intent", None)
    print("intent_group in the ami_drive:", intent_group)
    if intent_group == "general_conversation":
        return handle_general_conversation(intent, sub_intent,user_message, user_context)

    elif intent_group == "sales_related":
        return handle_sales(user_message, user_context,company_goal,product_info)

    elif intent_group == "after_sales":
        return "Post-sales support"

    else:
        return "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu r√µ c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n kh√¥ng?"

def handle_general_conversation(intent, sub_intent, user_message, user_context):
    # L·∫•y th√¥ng tin kh√°ch h√†ng t·ª´ user_context
    customer_info = user_context.get("customer_info", {})
    chat_history = user_context.get("chat_history", [])

    # Append the new user message to the chat history
    chat_history.append(f"User: {user_message}")
    user_context["chat_history"] = chat_history  

    # **Danh s√°ch th√¥ng tin c·∫ßn thu th·∫≠p**
    required_fields = ["name", "age", "occupation", "interests"]
    missing_fields = [field for field in required_fields if not customer_info.get(field)]

    # **Ch·ªâ h·ªèi t√™n n·∫øu th·∫≠t s·ª± ch∆∞a c√≥**
    if "name" in missing_fields:
        probing_prompt = f"""
        L·ªãch s·ª≠ h·ªôi tho·∫°i: {', '.join(chat_history)}  # Join the list into a string for display
        Th√¥ng tin kh√°ch h√†ng hi·ªán c√≥: {customer_info}
        B·∫°n l√† m·ªôt tr·ª£ l√Ω AI. Kh√°ch h√†ng ch∆∞a cung c·∫•p t√™n. H√£y ƒë·∫∑t m·ªôt c√¢u h·ªèi l·ªãch s·ª± ƒë·ªÉ h·ªèi t√™n.
        """
        return llm.invoke(probing_prompt).content

    # **N·∫øu ƒë√£ c√≥ t√™n nh∆∞ng c√≤n thi·∫øu th√¥ng tin kh√°c ‚Üí H·ªèi ti·∫øp th√¥ng tin c√≤n thi·∫øu**
    if missing_fields:
        probing_prompt = f"""
        L·ªãch s·ª≠ h·ªôi tho·∫°i: {', '.join(chat_history)}  # Join the list into a string for display
        Th√¥ng tin kh√°ch h√†ng hi·ªán c√≥: {customer_info}
        Th√¥ng tin c√≤n thi·∫øu: {missing_fields}
        H√£y ƒë·∫∑t m·ªôt c√¢u h·ªèi t·ª± nhi√™n ƒë·ªÉ khai th√°c m·ªôt trong c√°c th√¥ng tin c√≤n thi·∫øu m√† kh√¥ng l√†m kh√°ch h√†ng kh√≥ ch·ªãu.
        """
        return llm.invoke(probing_prompt).content

    # **N·∫øu ƒë√£ c√≥ ƒë·ªß th√¥ng tin ‚Üí Tr·∫£ l·ªùi theo ng·ªØ c·∫£nh**
    response_prompt = f"""
    T√≥m t·∫Øt h·ªôi tho·∫°i: {', '.join(chat_history)}  # Join the list into a string for display
    Th√¥ng tin kh√°ch h√†ng: {customer_info}
    C√¢u kh√°ch h√†ng v·ª´a h·ªèi: {user_message}
    H√£y ph·∫£n h·ªìi m·ªôt c√°ch t·ª± nhi√™n, ph√π h·ª£p v·ªõi th√¥ng tin kh√°ch h√†ng, gi·ªØ cu·ªôc tr√≤ chuy·ªán m∆∞·ª£t m√†.
    """
    extract_prompt = f"""
    H·ªôi tho·∫°i: {', '.join(chat_history)}  # Join the list into a string for display
    Th√¥ng tin hi·ªán c√≥: {user_context.get("customer_info", {})}
    H√£y c·∫≠p nh·∫≠t th√¥ng tin kh√°ch h√†ng d·ª±a tr√™n h·ªôi tho·∫°i m·ªõi. 
    Ch√∫ √Ω: N·∫øu ƒë√£ c√≥ th√¥ng tin, kh√¥ng ƒë∆∞·ª£c l√†m m·∫•t th√¥ng tin c≈©. Ch·ªâ b·ªï sung ph·∫ßn c√≤n thi·∫øu.
    """

    return llm.invoke(response_prompt).content
    
    
def handle_sales(user_message, user_context,company_goal,product_info):
    """
    X·ª≠ l√Ω tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng, x√°c ƒë·ªãnh m·ª•c ti√™u, t·∫°o Best_map v√† d·∫´n d·∫Øt h·ªôi tho·∫°i.
    """
    # B∆∞·ªõc 1: L·∫•y th√¥ng tin kh√°ch h√†ng t·ª´ user_context
    customer_info = user_context.get("customer_info", {})
    chat_history = user_context.get("chat_history", [])
    chat_history.append(f"User: {user_message}")
    user_context["chat_history"] = chat_history  # C·∫≠p nh·∫≠t l·ªãch s·ª≠ h·ªôi tho·∫°i
    
    # B∆∞·ªõc 2: X√°c ƒë·ªãnh customer_stage t·ª´ l·ªãch s·ª≠ h·ªôi tho·∫°i
    customer_stage = get_customer_stage(chat_history)
    user_context["customer_stage"] = customer_stage
    print("customer_stage:", customer_stage)

    # B∆∞·ªõc 3: X√°c ƒë·ªãnh m·ª•c ti√™u h·ªôi tho·∫°i
    cg = get_conversation_goal(customer_info, user_message, customer_stage)
    print("conversation_goal in handle_user_message:", cg)

    # B∆∞·ªõc 3: C·∫≠p nh·∫≠t customer_info v·ªõi customer_stage
    customer_info["customer_stage"] = customer_stage
    # B∆∞·ªõc 4: T·∫°o Best_map
    best_map = create_best_map(cg, customer_info,company_goal,product_info)
    print("best_map in handle_user_message:", best_map)

    response = generate_response(best_map, company_goal, customer_info)
    return response

def get_customer_stage(chat_history, company_goal="kh√°ch chuy·ªÉn kho·∫£n"):
    """
    D√πng LLM ƒë·ªÉ x√°c ƒë·ªãnh giai ƒëo·∫°n c·ªßa kh√°ch h√†ng d·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i.
    """
    prompt = f"""
    B·∫°n l√† m·ªôt AI t∆∞ v·∫•n b√°n h√†ng. D∆∞·ªõi ƒë√¢y l√† l·ªãch s·ª≠ h·ªôi tho·∫°i gi·ªØa nh√¢n vi√™n v√† kh√°ch h√†ng:
    {chat_history}
    
    C√¥ng ty c√≥ m·ª•c ti√™u cu·ªëi c√πng l√† '{company_goal}'.
    D·ª±a v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i, h√£y x√°c ƒë·ªãnh kh√°ch h√†ng ƒëang ·ªü giai ƒëo·∫°n n√†o trong h√†nh tr√¨nh n√†y:
    - Awareness (Nh·∫≠n th·ª©c)
    - Interest (Quan t√¢m)
    - Consideration (C√¢n nh·∫Øc)
    - Decision (Quy·∫øt ƒë·ªãnh)
    - Action (Chuy·ªÉn kho·∫£n)
    
    Ch·ªâ tr·∫£ v·ªÅ m·ªôt trong c√°c giai ƒëo·∫°n tr√™n m√† kh√¥ng c√≥ b·∫•t k·ª≥ gi·∫£i th√≠ch n√†o.
    """

    response= llm.invoke(prompt).content
    return response.strip()


def customer_emotion(chat_history):
   
    prompt = f"""
    B·∫°n l√† m·ªôt chuy√™n gia t√¢m l√Ω tinh t·∫ø. H√£y ph√°t hi·ªán c·∫£m x√∫c hi·ªán t·∫°i c·ªßa kh√°ch h√†ng d·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i:
    {chat_history}
    Ch·ªâ tr·∫£ v·ªÅ tr·∫°ng th√°i c·∫£m x√∫c m√† kh√¥ng gi·∫£i th√≠ch th√™m
    """
    response= llm.invoke(prompt).content
    return response.strip()

def get_customer_info(chat_history):
    """
    D√πng LLM ƒë·ªÉ ph√¢n t√≠ch l·ªãch s·ª≠ h·ªôi tho·∫°i v√† tr√≠ch xu·∫•t th√¥ng tin kh√°ch h√†ng.
    """
    # Join the chat history into a single string for processing
    chat_history_str = "\n".join(user_context["chat_history"])
    print("Formatted chat_history_str:", repr(chat_history_str))

    
 

    prompt = f"""
D∆∞·ªõi ƒë√¢y l√† l·ªãch s·ª≠ h·ªôi tho·∫°i gi·ªØa AI v√† kh√°ch h√†ng:
{chat_history_str}

H√£y c·ªë g·∫Øng suy lu·∫≠n th√¥ng tin t·ª´ cu·ªôc tr√≤ chuy·ªán n√†y.
N·∫øu kh√°ch h√†ng ƒë·ªÅ c·∫≠p ƒë·∫øn m·ªôt t√™n ri√™ng, gi·∫£ ƒë·ªãnh ƒë√≥ l√† "name".
N·∫øu kh√°ch h√†ng n√≥i v·ªÅ c√¥ng vi·ªác c·ªßa h·ªç, ƒë√≥ l√† "occupation".
N·∫øu ch∆∞a c√≥ ƒë·ªß th√¥ng tin, h√£y d·ª± ƒëo√°n d·ª±a tr√™n ng·ªØ c·∫£nh ho·∫∑c ƒë·ªÉ tr·ªëng.

Tr·∫£ v·ªÅ m·ªôt JSON v·ªõi c√°c tr∆∞·ªùng:
- name (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
- age (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
- gender (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
- occupation (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
- interests (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
- purchase_history (n·∫øu c√≥ th·ªÉ suy lu·∫≠n)
"""


    response = llm.invoke(prompt).content  # G·ªçi LLM ƒë·ªÉ ph√¢n t√≠ch
    print("response in the get_customer_info:", response)

    try:
        # Clean the JSON string if it has extra formatting
        json_start = response.find("{")
        json_end = response.rfind("}") + 1
        clean_json = response[json_start:json_end]

        # Parse JSON
        extracted_info = json.loads(clean_json)
        print("Extracted customer info:", extracted_info)

        return extracted_info
    except json.JSONDecodeError as e:
        print("JSON decoding error:", e)
        return {}

def update_customer_info(current_info, new_info):
    """
    C·∫≠p nh·∫≠t th√¥ng tin kh√°ch h√†ng v·ªõi d·ªØ li·ªáu m·ªõi m√† kh√¥ng l√†m m·∫•t th√¥ng tin c≈©.
    """
    for key, value in new_info.items():
        if value:  # Ch·ªâ c·∫≠p nh·∫≠t n·∫øu c√≥ th√¥ng tin m·ªõi
            current_info[key] = value
    
    # Ki·ªÉm tra n·∫øu v·∫´n c√≤n missing fields
    missing_fields = [key for key, value in current_info.items() if not value]
    if missing_fields:
        current_info["status"] = "missing_info"
        current_info["missing_fields"] = missing_fields
    else:
        current_info["status"] = "completed"

    return current_info

def chat_pipeline(user_message, chat_history, customer_info, llm):
   
    # 1. Tr√≠ch xu·∫•t th√¥ng tin t·ª´ l·ªãch s·ª≠ chat
    new_info = customer_info(chat_history, llm)
    print("new_info:", new_info)

    # 2. C·∫≠p nh·∫≠t th√¥ng tin kh√°ch h√†ng
    customer_info = update_customer_info(customer_info, new_info)
    print("customer_info:", customer_info)
    # 3. Ki·ªÉm tra xem ƒë√£ ƒë·ªß th√¥ng tin ch∆∞a
    if customer_info["status"] == "missing_info":
        missing = customer_info["missing_fields"]
        next_question = ask_for_missing_info(missing)
        return next_question, customer_info
    
    # 4. N·∫øu ƒë√£ ƒë·ªß th√¥ng tin, ti·∫øp t·ª•c h·ªôi tho·∫°i
    response = continue_conversation(user_message, customer_info, llm)
    return response, customer_info
def ask_for_missing_info(missing_fields):
    """
    Sinh c√¢u h·ªèi ƒë·ªÉ ti·∫øp t·ª•c ho√†n thi·ªán th√¥ng tin kh√°ch h√†ng.
    """
    questions = {
        "name": "B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt t√™n c·ªßa b·∫°n kh√¥ng?",
        "age": "B·∫°n bao nhi√™u tu·ªïi?",
        "gender": "B·∫°n l√† nam hay n·ªØ?",
        "occupation": "B·∫°n ƒëang l√†m ngh·ªÅ g√¨?",
        "interests": "B·∫°n quan t√¢m ƒë·∫øn lƒ©nh v·ª±c n√†o?",
        "purchase_history": "B·∫°n ƒë√£ t·ª´ng mua s·∫£n ph·∫©m n√†o t∆∞∆°ng t·ª± ch∆∞a?"
    }
    for field in missing_fields:
        if field in questions:
            return questions[field]  # H·ªèi l·∫ßn l∆∞·ª£t t·ª´ng c√¢u
    return "H√£y cho t√¥i bi·∫øt th√™m v·ªÅ b·∫°n!"  # N·∫øu kh√¥ng c√≥ c√¢u h·ªèi c·ª• th·ªÉ
def continue_conversation(user_message, customer_info, llm):
    """
    Ti·∫øp t·ª•c h·ªôi tho·∫°i d·ª±a tr√™n th√¥ng tin kh√°ch h√†ng v√† m·ª•c ti√™u h·ªôi tho·∫°i.
    """
    cg = get_conversation_goal(user_message, customer_info)
    
    best_map = create_best_map(cg, customer_info)
    
    response = llm.generate(f"D·ª±a v√†o m·ª•c ti√™u '{cg}', h√£y ph·∫£n h·ªìi: {user_message}")
    
    return response

def get_conversation_goal(customer_info, user_message, customer_stage):
    """
    X√°c ƒë·ªãnh m·ª•c ti√™u h·ªôi tho·∫°i d·ª±a tr√™n th√¥ng tin kh√°ch h√†ng, n·ªôi dung tin nh·∫Øn v√† giai ƒëo·∫°n kh√°ch h√†ng.
    """
    print("user_message in the determine_conversation_goal:", user_message)
    print("customer_info in the determine_conversation_goal:", customer_info)
    print("customer_stage in the determine_conversation_goal:", customer_stage)

    # N·∫øu th√¥ng tin kh√°ch c√≤n thi·∫øu, c·∫ßn ti·∫øp t·ª•c h·ªèi ƒë·ªÉ ho√†n ch·ªânh
    if "missing_fields" in customer_info and len(customer_info["missing_fields"]) > 0:
        return "Kh·ªüi t·∫°o h·ªôi tho·∫°i chung"

    # X√°c ƒë·ªãnh m·ª•c ti√™u ti·∫øp theo b·∫±ng c√°ch suy lu·∫≠n t·ª´ company_goal
    prompt = f"""
    D·ª±a tr√™n giai ƒëo·∫°n kh√°ch h√†ng trong h√†nh tr√¨nh mua h√†ng: "{customer_stage}", 
    v√† tin nh·∫Øn: "{user_message}", h√£y x√°c ƒë·ªãnh b∆∞·ªõc h·ª£p l√Ω ti·∫øp theo ƒë·ªÉ d·∫´n kh√°ch h√†ng ƒë·∫øn m·ª•c ti√™u "Chuy·ªÉn kho·∫£n".
    
    Tr·∫£ v·ªÅ ch·ªâ m·ªôt m·ª•c ti√™u h·ªôi tho·∫°i c·ª• th·ªÉ (kh√¥ng gi·∫£i th√≠ch), v√≠ d·ª•: "Gi·ªõi thi·ªáu s·∫£n ph·∫©m", "Thuy·∫øt ph·ª•c kh√°ch h√†ng", "H∆∞·ªõng d·∫´n thanh to√°n".
    """

    response = llm.invoke(prompt).content  # G·ªçi LLM ƒë·ªÉ ph√¢n t√≠ch


    return response.strip()

def create_best_map(conversation_goal, customer_info, company_goal, product_info):
    """
    S·ª≠ d·ª•ng LLM ƒë·ªÉ suy lu·∫≠n Best_map ph√π h·ª£p d·ª±a tr√™n conversation_goal, customer_info v√† company_goal.
    """
    prompt = f"""
    üõí Kh√°ch h√†ng ƒëang ·ªü giai ƒëo·∫°n: "{customer_info.get('customer_stage', 'Unknown')}"
    üéØ M·ª•c ti√™u h·ªôi tho·∫°i: "{conversation_goal}"
    üèÜ M·ª•c ti√™u cu·ªëi c√πng c·ªßa c√¥ng ty: "{company_goal}"
    üë§ Th√¥ng tin kh√°ch h√†ng: {customer_info}
    üì¶ Th√¥ng tin s·∫£n ph·∫©m c√¥ng ty: {product_info}

    ‚úÖ H√£y t·∫°o m·ªôt h∆∞·ªõng d·∫´n ph·∫£n h·ªìi t·ªët nh·∫•t (Best_map) gi√∫p nh√¢n vi√™n b√°n h√†ng n√≥i chuy·ªán h·ª£p l√Ω v√† h∆∞·ªõng kh√°ch h√†ng ƒë·∫øn {company_goal}.
    ‚úÖ ƒêi·ªÅu ch·ªânh ph·∫£n h·ªìi d·ª±a tr√™n c·∫£m x√∫c v√† giai ƒëo·∫°n c·ªßa kh√°ch h√†ng:
    - N·∫øu ch∆∞a bi·∫øt t√™n kh√°ch h√†ng, h√£y h·ªèi t√™n kh√°ch h√†ng tr∆∞·ªõc.
    - N·∫øu kh√°ch h√†ng c√≤n ph√¢n v√¢n, h√£y nh·∫•n m·∫°nh l·ª£i √≠ch c·ªßa s·∫£n ph·∫©m.
    - N·∫øu kh√°ch h√†ng c√≥ h·ª©ng th√∫, h√£y g·ª£i m·ªü m·ªôt l√Ω do m·∫°nh m·∫Ω ƒë·ªÉ h√†nh ƒë·ªông ngay.
    - N·∫øu kh√°ch h√†ng c√≥ lo ng·∫°i, h√£y tr·∫•n an v√† cung c·∫•p th√¥ng tin h·ªó tr·ª£.

    üé§ N·∫øu bi·∫øt t√™n kh√°ch h√†ng, h√£y x∆∞ng h√¥ th√¢n thi·ªán.
    üì¢ Tr·∫£ v·ªÅ m·ªôt ƒëo·∫°n vƒÉn ng·∫Øn, kh√¥ng qu√° 3 c√¢u, v·ªõi phong c√°ch giao ti·∫øp th∆∞·ªùng th·ª©c (casual).
    """
    response = llm.invoke(prompt).content  # G·ªçi OpenAI ho·∫∑c m√¥ h√¨nh AI kh√°c
    return response.strip()


def search_sales_skills(query_text, max_skills=3):
    """ 
    Truy v·∫•n k·ªπ nƒÉng t·ª´ Pinecone v·ªõi ƒë·ªô ch√≠nh x√°c cao h∆°n. 
    """
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-large", dimensions=1536)
    query_embedding = embedding_model.embed_query(query_text)

    index = pc.Index(index_name)
    response = index.query(
        vector=query_embedding,
        top_k=max_skills,
        include_metadata=True  
    )

    skills = []
    if response and "matches" in response:
        for match in response["matches"]:
            skill_text = match.get("metadata", {}).get("content")
            if skill_text:
                skills.append(skill_text)

    return skills if skills else ["Kh√¥ng t√¨m th·∫•y k·ªπ nƒÉng ph√π h·ª£p."]



chain = (
    RunnablePassthrough.assign(
        history=lambda _: memory.load_memory_variables({}).get("history", []),
        products=lambda x: retrieve_product(x["user_input"]),
        sales_skills=lambda x: ", ".join(search_sales_skills(x["user_input"], max_skills=3)),  # L·∫•y k·ªπ nƒÉng t·ª´ Pinecone
        user_style=lambda _: "l·ªãch s·ª±"
    )  
    | prompt
    | llm
)



def ami_selling(user_message):
    """
    H√†m ch√≠nh x·ª≠ l√Ω h·ªôi tho·∫°i b√°n h√†ng c·ªßa Ami.
    """
    global user_context  # Declare user_context as global

    print("user_message in the ami_selling:", user_message)

    # Append the new user message to the chat history
    print("Before appending message:", user_context["chat_history"])
    user_context["chat_history"].append(f"User: {user_message}")
    print("After appending message:", user_context["chat_history"])


    # Tr√≠ch xu·∫•t th√¥ng tin kh√°ch h√†ng
    extracted_info = get_customer_info(user_message)
    print("extracted_info in the ami_selling:", extracted_info)

    # C·∫≠p nh·∫≠t user_context v·ªõi customer_info m·ªõi
    user_context["customer_info"] = update_customer_info(user_context.get("customer_info", {}), extracted_info)

    print("Updated user_context:", user_context)

    company_goal = "Kh√°ch chuy·ªÉn kho·∫£n"
    product_info = retrieve_product(user_message)

    # G·ªçi handle_user_message ƒë·ªÉ l·∫•y ph·∫£n h·ªìi ch√≠nh theo Best_map
    response = ami_drive(user_message, user_context, company_goal, product_info)

    return response

def generate_response(best_map, company_goal, customer_info):
    """
    Sinh ph·∫£n h·ªìi d·ª±a tr√™n Best_map + h∆∞·ªõng kh√°ch h√†ng ƒë·∫øn company_goal.
    """
    prompt = f"""
    Kh√°ch h√†ng: {customer_info}
    Best_map: "{best_map}"
    Company_goal: "{company_goal}"

    H√£y t·∫°o m·ªôt ph·∫£n h·ªìi t·ª± nhi√™n, th√¢n thi·ªán, d·∫´n d·∫Øt kh√°ch h√†ng theo Best_map v√† h∆∞·ªõng h·ªç ƒë·∫øn {company_goal}.
    """

    response = llm.invoke(prompt).content  # G·ªçi OpenAI ho·∫∑c m√¥ h√¨nh AI kh√°c ƒë·ªÉ sinh ph·∫£n h·ªìi
    return response.strip()